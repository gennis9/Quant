{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jFVxtlshAsY"
   },
   "source": [
    "**Visual Graph Learning with GCN for Financial Forecasting: CNN-Based\n",
    "OHLC Image Embeddings and Market Topology**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_SmqMYjAkBf"
   },
   "source": [
    "# **CODE 2 : CNN_Test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gk5HwGcZiHM4"
   },
   "source": [
    "# ğŸ› ï¸ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B1kMrknjiGhZ"
   },
   "outputs": [],
   "source": [
    "# @title Import packages\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pickle as pkl\n",
    "from collections import OrderedDict\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17816,
     "status": "ok",
     "timestamp": 1763498254807,
     "user": {
      "displayName": "è³´å† éœ–",
      "userId": "06174036697869130378"
     },
     "user_tz": -480
    },
    "id": "R83UmcWxO0sw",
    "outputId": "f96c2389-a482-471a-ef68-04432e34b1bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# @title Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NoKITXnSglph"
   },
   "source": [
    "# CNN æ¨¡å‹å®šç¾©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlYa0hgFgX3U"
   },
   "outputs": [],
   "source": [
    "# @title 20-day CNN Architecture\n",
    "# (BasicBlock, Bottleneck, CNN20d)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet åŸºç¤æ®˜å·®å€å¡Š (å°æ‡‰åœ–ä¸­çš„ 5x3 -> 5x3)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        # --- ä¸»è·¯å¾‘ (Main Path) ---\n",
    "        self.main_path = nn.Sequential(\n",
    "            # 1. 5x3 Conv\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=(5, 3), stride=1, padding=(2, 1)),\n",
    "            # 2. 5x3 Conv\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=(5, 3), stride=1, padding=(2, 1)),\n",
    "\n",
    "        )\n",
    "\n",
    "        # --- è·³æ¥ (Shortcut Path) ---\n",
    "        self.shortcut = nn.Sequential() # é»˜èªç‚º Identity\n",
    "\n",
    "        # å¦‚æœè¼¸å…¥/è¼¸å‡ºé€šé“æ•¸ä¸åŒ (ä¾‹å¦‚ 64 -> 128)ï¼Œ\n",
    "        # æˆ‘å€‘éœ€è¦ä¸€å€‹ 1x1 å·ç©ä¾†åŒ¹é…ç¶­åº¦\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        main_out = self.main_path(x)\n",
    "        shortcut_out = self.shortcut(x)\n",
    "        out = main_out + shortcut_out # ä¸åŠ  final activation\n",
    "        return out\n",
    "\n",
    "class BottleneckBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet ç“¶é ¸æ®˜å·®å€å¡Š (å°æ‡‰åœ–ä¸­çš„ 1x1 -> 5x3 -> 1x1)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, squeeze_channels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): è¼¸å…¥é€šé“æ•¸\n",
    "            out_channels (int): è¼¸å‡ºé€šé“æ•¸ (ä¾‹å¦‚ 512)\n",
    "            squeeze_channels (int): ç“¶é ¸å£“ç¸®å¾Œçš„é€šé“æ•¸ (ä¾‹å¦‚ 128)\n",
    "        \"\"\"\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "\n",
    "        # --- ä¸»è·¯å¾‘ (Main Path) ---\n",
    "        self.main_path = nn.Sequential(\n",
    "            # 1. 1x1 Conv (å£“ç¸®)\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(in_channels, squeeze_channels, kernel_size=1, stride=1, padding=0),\n",
    "\n",
    "\n",
    "            # 2. 5x3 Conv (æ‚¨çš„æ ¸å¿ƒå·ç©)\n",
    "            nn.BatchNorm2d(squeeze_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(squeeze_channels, squeeze_channels, kernel_size=(5, 3), stride=1, padding=(2, 1)),\n",
    "\n",
    "\n",
    "            # 3. 1x1 Conv (æ“´å±•)\n",
    "            nn.BatchNorm2d(squeeze_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(squeeze_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "        )\n",
    "\n",
    "        # --- è·³æ¥ (Shortcut Path) ---\n",
    "        self.shortcut = nn.Sequential() # é»˜èªç‚º Identity\n",
    "\n",
    "        # å¦‚æœè¼¸å…¥/è¼¸å‡ºé€šé“æ•¸ä¸åŒ (ä¾‹å¦‚ 256 -> 512)ï¼Œ\n",
    "        # æˆ–é€™æ˜¯ç¬¬ä¸€å€‹å€å¡Šï¼Œæˆ‘å€‘éœ€è¦ä¸€å€‹ 1x1 å·ç©ä¾†åŒ¹é…ç¶­åº¦\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        main_out = self.main_path(x)\n",
    "        shortcut_out = self.shortcut(x)\n",
    "        out = main_out + shortcut_out # ä¸åŠ  final activation\n",
    "        return out\n",
    "\n",
    "class CNN20d(nn.Module):\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.01)\n",
    "\n",
    "    def _make_layer(self, block_type, in_channels, out_channels, num_blocks, squeeze_channels=None):\n",
    "        \"\"\"\n",
    "        å»ºç«‹ä¸€å€‹åŒ…å« N å€‹æ®˜å·®å€å¡Šçš„å±¤ (Stage)\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "\n",
    "        # ç¬¬ä¸€å€‹å€å¡Šï¼šè™•ç†è¼¸å…¥/è¼¸å‡ºé€šé“æ•¸ä¸åŒçš„æƒ…æ³\n",
    "        if block_type == BottleneckBlock:\n",
    "            layers.append(block_type(in_channels, out_channels, squeeze_channels))\n",
    "        else: # BasicBlock\n",
    "            layers.append(block_type(in_channels, out_channels))\n",
    "\n",
    "        # å‰©é¤˜çš„ (num_blocks - 1) å€‹å€å¡Š\n",
    "        for _ in range(1, num_blocks):\n",
    "            if block_type == BottleneckBlock:\n",
    "                # å¾ŒçºŒå€å¡Šçš„ in_channels ç­‰æ–¼ out_channels\n",
    "                layers.append(block_type(out_channels, out_channels, squeeze_channels))\n",
    "            else: # BasicBlock\n",
    "                layers.append(block_type(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def __init__(self, num_classes=2, regression_outputs=1):\n",
    "        super(CNN20d, self).__init__()\n",
    "\n",
    "        # 1. Stem (è–å¹¹å±¤) - æ ¹æ“šæ‚¨çš„ H/W èª¿æ•´\n",
    "        self.conv1 = nn.Sequential(\n",
    "            # ä½¿ç”¨ 5x3 å·ç©æ ¸ (kernel_size=(5, 3)) æ›´é©åˆæ‚¨çš„éæ–¹å½¢è¼¸å…¥\n",
    "            # padding=(2, 1) å¯ä»¥åœ¨ stride=1 æ™‚ä¿æŒ H å’Œ W å°ºå¯¸ä¸è®Š\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(1, 64, kernel_size=(5, 3), stride=1, padding=(2, 1))\n",
    "        )\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d((2, 1)) # ç¬¬ä¸€å€‹ MaxPool\n",
    "\n",
    "        # 2. Stage 1 - å°æ‡‰åœ–ä¸­ x2 (BasicBlock, 64)\n",
    "        self.stage1 = self._make_layer(BasicBlock, 64, 64, num_blocks=2)\n",
    "\n",
    "        # 3. Stage 2 - å°æ‡‰åœ–ä¸­ x2 (BasicBlock, 128)\n",
    "        self.stage2 = self._make_layer(BasicBlock, 64, 128, num_blocks=2)\n",
    "\n",
    "        # 4. Stage 3 - å°æ‡‰åœ–ä¸­ x2 (BasicBlock, 256)\n",
    "        self.stage3 = self._make_layer(BottleneckBlock, 128, 256, num_blocks=2, squeeze_channels=64)\n",
    "        self.pool2 = nn.MaxPool2d((2, 1)) # ç¬¬äºŒå€‹ MaxPool\n",
    "\n",
    "        # 5. Stage 4 - å°æ‡‰åœ–ä¸­ x2 (Bottleneck, 128 -> 512)\n",
    "        # ç“¶é ¸å£“ç¸®é€šé“æ•¸ç‚º 128ï¼Œè¼¸å‡ºé€šé“æ•¸ç‚º 512\n",
    "        self.stage4 = self._make_layer(BottleneckBlock, 256, 512, num_blocks=2, squeeze_channels=128)\n",
    "\n",
    "\n",
    "        # 6. Final Pooling (å…¨å±€å¹³å‡æ± åŒ–)\n",
    "        # é€™æœƒå°‡ (Batch, 512, H, W) -> (Batch, 512, 1, 1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # 7. Dropout å’Œå…¨é€£æ¥å±¤\n",
    "        self.DropOut = nn.Dropout(p=0.5)\n",
    "\n",
    "        # flattened_size ç¾åœ¨å›ºå®šç‚ºæœ€å¾Œçš„é€šé“æ•¸ 512\n",
    "        self.classification_fc = nn.Linear(512, num_classes)\n",
    "        self.classification_activation = nn.Softmax(dim=1)\n",
    "\n",
    "        # self.regression_fc = nn.Linear(512, regression_outputs)\n",
    "        # self.regression_activation = nn.Identity()\n",
    "\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x, task_type='classification', embedding=None, return_params=False):\n",
    "        # ç¢ºä¿è¼¸å…¥æ˜¯ (Batch, Channels, H, W)\n",
    "        # åŸå§‹ç¢¼æ˜¯ (Batch, H, W)ï¼Œæ‰€ä»¥ unsqueeze(1) è®Šæˆ (Batch, 1, H, W)\n",
    "        x = x.unsqueeze(1).to(torch.float32)\n",
    "\n",
    "        outputs = {}\n",
    "\n",
    "        # æŒ‰ç…§æ¶æ§‹åœ–é †åºåŸ·è¡Œ\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        outputs['conv1'] = x\n",
    "\n",
    "        x = self.stage1(x)\n",
    "        outputs['stage1'] = x\n",
    "\n",
    "        x = self.stage2(x)\n",
    "        outputs['stage2'] = x\n",
    "\n",
    "        x = self.stage3(x)\n",
    "        x = self.pool2(x)\n",
    "        outputs['stage3'] = x\n",
    "\n",
    "        x = self.stage4(x)\n",
    "        outputs['stage4'] = x\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        # x shape: (Batch, 512, 1, 1)\n",
    "\n",
    "        # å±•å¹³ (Flatten)\n",
    "        x = torch.flatten(x, 1) # (Batch, 512)\n",
    "\n",
    "        x = self.DropOut(x)\n",
    "        outputs['flatten'] = x\n",
    "\n",
    "        if embedding in outputs:\n",
    "            return outputs[embedding]\n",
    "\n",
    "        # Task-specific output (é€™éƒ¨åˆ†é‚è¼¯èˆ‡æ‚¨åŸç¢¼ç›¸åŒ)\n",
    "        if task_type == 'classification':\n",
    "            x = self.classification_fc(x)\n",
    "            x = self.classification_activation(x)\n",
    "        # elif task_type == 'regression':\n",
    "        #     x = self.regression_fc(x)\n",
    "        #     x = self.regression_activation(x)\n",
    "\n",
    "        # (é€™éƒ¨åˆ†é‚è¼¯èˆ‡æ‚¨åŸç¢¼ç›¸åŒ)\n",
    "        if return_params and embedding:\n",
    "            return {\n",
    "                'params': dict(self.named_parameters()),\n",
    "                'output': outputs[embedding]\n",
    "            }\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvAaCyGZfAsj"
   },
   "source": [
    "# è¼”åŠ©å‡½å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNbVx0v6e8OE"
   },
   "outputs": [],
   "source": [
    "def make_y_labels(y, mode=\"top_k\", top_k=9, threshold=0.003):\n",
    "    \"\"\"\n",
    "    æ ¹æ“š 'mode' åƒæ•¸æ±ºå®š label çš„ç”Ÿæˆæ–¹å¼ï¼š\n",
    "    - mode=\"top_k\":    (é è¨­) ä¾æ“šåŒä¸€å¤©çš„å ±é…¬ç‡æ’åï¼Œæ¨™å‡º top_k ç‚º 1ï¼Œå…¶é¤˜ç‚º 0ã€‚\n",
    "    - mode=\"threshold\": ä¾æ“š 'Return_5D' >= threshold (é è¨­ 0.003) ç‚º 1ï¼Œå…¶é¤˜ç‚º 0ã€‚\n",
    "    \"\"\"\n",
    "    y = y.copy()\n",
    "    y.columns = ['TradeDate', 'Ticker', 'Return_5D']\n",
    "\n",
    "    if mode == \"top_k\":   # top_k=9\n",
    "        y['rank'] = y.groupby('TradeDate')['Return_5D'].rank(ascending=False, method='first')\n",
    "        y['label'] = (y['rank'] <= top_k).astype(int)\n",
    "    elif mode == \"threshold\":   # threshold=0.003\n",
    "        y['label'] = (y['Return_5D'] >= threshold).astype(int)\n",
    "    else:\n",
    "        raise ValueError(f\"æœªçŸ¥çš„ mode: '{mode}'\")\n",
    "\n",
    "    # è½‰æ›æˆ tensor\n",
    "    y_tensor = torch.tensor(y['label'].values, dtype=torch.long)\n",
    "    dataset = TensorDataset(y_tensor)\n",
    "\n",
    "    return dataset, y # å¤šå›å‚³ä¸€å€‹ y dataframe ä»¥ä¾¿å¾ŒçºŒåˆä½µè³‡æ–™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "padwewHOO-bc"
   },
   "source": [
    "# ğŸ› ï¸ è¼‰å…¥è¨“ç·´å¥½çš„CNNæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 149,
     "status": "ok",
     "timestamp": 1763501912669,
     "user": {
      "displayName": "è³´å† éœ–",
      "userId": "06174036697869130378"
     },
     "user_tz": -480
    },
    "id": "sNl8qdZLPAfm",
    "outputId": "d2cc8764-2deb-4be6-e723-92717faf269f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "è¼‰å…¥ CNN æ¬Šé‡å¾: /content/drive/MyDrive/Colab Notebooks/é›»è…¦è¦–è¦ºä¹‹æ·±åº¦å­¸ç¿’/final_project/CNN_model/2_cnn20d_final_lr1e-4_y3.pth\n",
      "âœ… CNN æ¨¡å‹è¼‰å…¥å®Œæˆä¸¦è¨­ç‚º eval() æ¨¡å¼ã€‚\n"
     ]
    }
   ],
   "source": [
    "# è·¯å¾‘è¨­å®š\n",
    "PROJECT_PATH = \"/content/drive/MyDrive/Colab Notebooks/é›»è…¦è¦–è¦ºä¹‹æ·±åº¦å­¸ç¿’/final_project\"\n",
    "\n",
    "# --- è¼‰å…¥CNNæ¨¡å‹ ---\n",
    "# CNN æ¨¡å‹è·¯å¾‘ (ä¾†è‡ªCNN_Draftè¼¸å‡ºçš„CNN_model)\n",
    "CNN_output_dir = os.path.join(PROJECT_PATH, \"CNN_model\")\n",
    "\n",
    "model_path = os.path.join(CNN_output_dir, \"2_cnn20d_final_lr1e-4_y3.pth\")\n",
    "device = torch.device('cuda')\n",
    "model = CNN20d().to(device)\n",
    "\n",
    "print(f\"\\nè¼‰å…¥ CNN æ¬Šé‡å¾: {model_path}\")\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval() # *** å‹™å¿…è¨­ç‚ºè©•ä¼°æ¨¡å¼ ***\n",
    "print(\"âœ… CNN æ¨¡å‹è¼‰å…¥å®Œæˆä¸¦è¨­ç‚º eval() æ¨¡å¼ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 621,
     "status": "ok",
     "timestamp": 1763501913304,
     "user": {
      "displayName": "è³´å† éœ–",
      "userId": "06174036697869130378"
     },
     "user_tz": -480
    },
    "id": "S2oYBSvwPzXd",
    "outputId": "c07214a0-46df-47cd-a662-d6115862162f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¾ /content/drive/MyDrive/Colab Notebooks/é›»è…¦è¦–è¦ºä¹‹æ·±åº¦å­¸ç¿’/final_project/Data_OHLC è¼‰å…¥è³‡æ–™...\n",
      "âœ… è³‡æ–™è¼‰å…¥å®Œæˆã€‚\n",
      "X_test shape: (5, 734, 28, 64, 60)\n",
      "y_test_0 shape: (20552, 3)\n",
      "y_test_1 shape: (20552, 3)\n",
      "y_test_2 shape: (20552, 3)\n",
      "y_test_3 shape: (20552, 3)\n",
      "y_test_4 shape: (20552, 3)\n"
     ]
    }
   ],
   "source": [
    "# --- è¼‰å…¥è³‡æ–™ ---\n",
    "# è³‡æ–™è·¯å¾‘ (Data_OHLC):\n",
    "_subfolder_o = os.path.join(PROJECT_PATH, \"Data_OHLC\")\n",
    "\n",
    "print(f\"å¾ {_subfolder_o} è¼‰å…¥è³‡æ–™...\")\n",
    "# X_test\n",
    "X_test = np.load(os.path.join(_subfolder_o, 'X_test.npy'))\n",
    "\n",
    "# CNN y\n",
    "y_test_0 = pd.read_csv(os.path.join(_subfolder_o, 'y_test_0.csv'))\n",
    "y_test_1 = pd.read_csv(os.path.join(_subfolder_o, 'y_test_1.csv'))\n",
    "y_test_2 = pd.read_csv(os.path.join(_subfolder_o, 'y_test_2.csv'))\n",
    "y_test_3 = pd.read_csv(os.path.join(_subfolder_o, 'y_test_3.csv'))\n",
    "y_test_4 = pd.read_csv(os.path.join(_subfolder_o, 'y_test_4.csv'))\n",
    "\n",
    "print(\"âœ… è³‡æ–™è¼‰å…¥å®Œæˆã€‚\")\n",
    "\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test_0 shape: {y_test_0.shape}\")\n",
    "print(f\"y_test_1 shape: {y_test_1.shape}\")\n",
    "print(f\"y_test_2 shape: {y_test_2.shape}\")\n",
    "print(f\"y_test_3 shape: {y_test_3.shape}\")\n",
    "print(f\"y_test_4 shape: {y_test_4.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfgw8bdgYnPo"
   },
   "source": [
    "# ğŸ› ï¸ é€²è¡Œæ¨è«–\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13SqgEbG30hn"
   },
   "source": [
    "**Mode=\"top_k\" or \"threshold\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 230953,
     "status": "ok",
     "timestamp": 1763502144261,
     "user": {
      "displayName": "è³´å† éœ–",
      "userId": "06174036697869130378"
     },
     "user_tz": -480
    },
    "id": "f7ZHBrlDYnli",
    "outputId": "22227d7b-47e0-436d-ccb4-f041264fd286"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== é–‹å§‹æ¨è«–èˆ‡ç¶­åº¦è½‰æ› ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Set 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 322/322 [00:46<00:00,  6.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 0 Accuracy : 0.5030\n",
      "Set 0 è¼¸å‡ºçŸ©é™£å½¢ç‹€: (28, 734)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Set 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 322/322 [00:45<00:00,  7.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 1 Accuracy : 0.5036\n",
      "Set 1 è¼¸å‡ºçŸ©é™£å½¢ç‹€: (28, 734)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Set 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 322/322 [00:46<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 2 Accuracy : 0.5023\n",
      "Set 2 è¼¸å‡ºçŸ©é™£å½¢ç‹€: (28, 734)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Set 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 322/322 [00:45<00:00,  7.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 3 Accuracy : 0.5014\n",
      "Set 3 è¼¸å‡ºçŸ©é™£å½¢ç‹€: (28, 734)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Set 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 322/322 [00:45<00:00,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 4 Accuracy : 0.5036\n",
      "Set 4 è¼¸å‡ºçŸ©é™£å½¢ç‹€: (28, 734)\n",
      "\n",
      "=== è™•ç†å®Œæˆ ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "y_test_list = [y_test_0, y_test_1, y_test_2, y_test_3, y_test_4]\n",
    "final_matrix_list = [] # ç”¨ä¾†å­˜ 5 å€‹ (28xT) çš„çŸ©é™£\n",
    "\n",
    "print(\"\\n=== é–‹å§‹æ¨è«–èˆ‡ç¶­åº¦è½‰æ› ===\")\n",
    "# Mode=\"top_k\"\n",
    "Mode=\"threshold\"\n",
    "for i in range(5):\n",
    "    # --- A. æº–å‚™ X ---\n",
    "    # X_test[i] shape: (734, 28, 64, 60) -> Flatten -> (20552, 64, 60)\n",
    "    # é€™è£¡ä¸ç”¨ unsqueeze(1) å› ç‚ºä½ çš„æ¨¡å‹ forward ç¬¬ä¸€è¡Œæœƒè‡ªå·±åš\n",
    "    flattened_X = X_test[i].reshape(-1, 64, 60)\n",
    "    tensor_X = torch.FloatTensor(flattened_X)\n",
    "\n",
    "    # --- B. æº–å‚™ y (ä½¿ç”¨ä½ çš„å‡½æ•¸) ---\n",
    "    current_y_df = y_test_list[i]\n",
    "    # å‘¼å« make_y_labels\n",
    "    dataset_for_model, df_with_info = make_y_labels(current_y_df, mode=Mode)\n",
    "    # å¾ TensorDataset å–å‡º tensor\n",
    "    tensor_y = dataset_for_model.tensors[0]\n",
    "\n",
    "    # --- C. DataLoader ---\n",
    "    dataset = TensorDataset(tensor_X, tensor_y)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    # --- D. æ¨è«– (Inference) ---\n",
    "    all_probs = [] # å­˜é æ¸¬æ©Ÿç‡ (0~1)\n",
    "    all_preds = [] # å­˜é æ¸¬é¡åˆ¥ (0/1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for b_x, b_y in tqdm(loader, desc=f\"Set {i}\"):\n",
    "            b_x = b_x.to(device)\n",
    "            outputs = model(b_x)\n",
    "\n",
    "            # outputs shape: (Batch, 2) -> [P(è·Œ), P(æ¼²)]\n",
    "            # å– index 1 ä»£è¡¨çœ‹å¥½çš„æ©Ÿç‡\n",
    "            probs = outputs[:, 1]\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    # è¨ˆç®—æº–ç¢ºåº¦ (Optional)\n",
    "    acc = accuracy_score(tensor_y.numpy(), all_preds)\n",
    "    print(f\"Set {i} Accuracy : {acc:.4f}\")\n",
    "\n",
    "    # --- E. è½‰æ›ç¶­åº¦è‡³ 28 * T ---\n",
    "    # æˆ‘å€‘é€šå¸¸é—œå¿ƒã€Œæ¨¡å‹é æ¸¬çš„æ©Ÿç‡ã€ï¼Œå› ç‚ºé€™æ±ºå®šäº†æˆ‘å€‘è¦è²·å“ªå¹¾æ”¯\n",
    "    # ç¸½ç­†æ•¸ 20552, è‚¡ç¥¨æ•¸ 28 -> å¤©æ•¸ = 20552 / 28 = 734\n",
    "    days = 734\n",
    "    stocks = 28\n",
    "\n",
    "    # 1. å…ˆè½‰æˆ (å¤©æ•¸, è‚¡ç¥¨æ•¸) -> (734, 28)\n",
    "    # é‚è¼¯ï¼šè³‡æ–™åŸæœ¬æ˜¯ Day1[28ç­†] -> Day2[28ç­†]...\n",
    "    prob_matrix_T_28 = np.array(all_probs).reshape(days, stocks)\n",
    "\n",
    "    # 2. è½‰ç½®æˆ (è‚¡ç¥¨æ•¸, å¤©æ•¸) -> (28, 734)\n",
    "    prob_matrix_28_T = prob_matrix_T_28.T\n",
    "\n",
    "    final_matrix_list.append(prob_matrix_28_T)\n",
    "    print(f\"Set {i} è¼¸å‡ºçŸ©é™£å½¢ç‹€: {prob_matrix_28_T.shape}\")\n",
    "print(\"\\n=== è™•ç†å®Œæˆ ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1763502144273,
     "user": {
      "displayName": "è³´å† éœ–",
      "userId": "06174036697869130378"
     },
     "user_tz": -480
    },
    "id": "Ip7oRfO7q-0d",
    "outputId": "59340cc0-cace-4222-fab3-e4c0813052a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.9662530e-01 6.0956785e-03 9.9812299e-01 8.4836207e-02 7.0229924e-09\n",
      " 3.3418366e-04 9.9952602e-01 9.9989378e-01 9.9851316e-01 2.1079030e-02\n",
      " 9.9999750e-01 8.8029772e-01 1.0000000e+00 6.6929375e-18 4.4953663e-09\n",
      " 9.9288321e-01 9.3628854e-01 5.3269707e-07 9.9999988e-01 3.2685800e-03\n",
      " 5.9017460e-03 6.6281966e-11 9.9998534e-01 6.8579107e-09 9.9994993e-01\n",
      " 1.2388439e-06 9.9999976e-01 1.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# ç¬¬0çµ„ï¼Œ28æ”¯è‚¡ç¥¨ï¼Œåœ¨ç¬¬0å¤©çš„æ©Ÿç‡\n",
    "print(final_matrix_list[0][:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBmbwx5qwyc8"
   },
   "source": [
    "# ğŸ› ï¸ å„²å­˜æª”æ¡ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1763502144290,
     "user": {
      "displayName": "è³´å† éœ–",
      "userId": "06174036697869130378"
     },
     "user_tz": -480
    },
    "id": "y9GRE01QweNn",
    "outputId": "fb25a679-54a3-45a1-8927-d91aff5a39ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²å„²å­˜ç‚º .npy æª”: /content/drive/MyDrive/Colab Notebooks/é›»è…¦è¦–è¦ºä¹‹æ·±åº¦å­¸ç¿’/final_project/CNN_testset_output/2_CNN_output_lr1e-4_y3.npy\n",
      "å„²å­˜å¾Œçš„å½¢ç‹€: (5, 28, 734)\n"
     ]
    }
   ],
   "source": [
    "# å„²å­˜æª”æ¡ˆ\n",
    "PROJECT_PATH = \"/content/drive/MyDrive/Colab Notebooks/é›»è…¦è¦–è¦ºä¹‹æ·±åº¦å­¸ç¿’/final_project\"\n",
    "\n",
    "CNN_testset_output = os.path.join(PROJECT_PATH, \"CNN_testset_output\")\n",
    "os.makedirs(CNN_testset_output, exist_ok=True)\n",
    "\n",
    "# 1. è½‰æ›æˆä¸€å€‹å¤§çš„ Numpy Array\n",
    "# å½¢ç‹€æœƒå¾ List è®Šæˆ (5, 28, 734) çš„ Array\n",
    "all_predictions = np.array(final_matrix_list)\n",
    "\n",
    "# 2. è¨­å®šæª”åèˆ‡å„²å­˜\n",
    "npy_path = os.path.join(CNN_testset_output, \"2_CNN_output_lr1e-4_y3.npy\")\n",
    "np.save(npy_path, all_predictions)\n",
    "\n",
    "print(f\"âœ… å·²å„²å­˜ç‚º .npy æª”: {npy_path}\")\n",
    "print(f\"å„²å­˜å¾Œçš„å½¢ç‹€: {all_predictions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1763502144319,
     "user": {
      "displayName": "è³´å† éœ–",
      "userId": "06174036697869130378"
     },
     "user_tz": -480
    },
    "id": "ZlH9dIFC4XeX",
    "outputId": "a2a90bac-c3dd-4c84-9b43-b5d0ea736ffb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "è¼‰å…¥ CNN æ¬Šé‡å¾: /content/drive/MyDrive/Colab Notebooks/é›»è…¦è¦–è¦ºä¹‹æ·±åº¦å­¸ç¿’/final_project/CNN_model/2_cnn20d_final_lr1e-4_y3.pth\n",
      "\n",
      "ä½¿ç”¨Mode : threshold\n",
      "âœ… å·²å„²å­˜ç‚º .npy æª”: /content/drive/MyDrive/Colab Notebooks/é›»è…¦è¦–è¦ºä¹‹æ·±åº¦å­¸ç¿’/final_project/CNN_testset_output/2_CNN_output_lr1e-4_y3.npy\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nè¼‰å…¥ CNN æ¬Šé‡å¾: {model_path}\")\n",
    "print(f\"\\nä½¿ç”¨Mode : {Mode}\")\n",
    "print(f\"âœ… å·²å„²å­˜ç‚º .npy æª”: {npy_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNPkFuUrZGAqi1evakKh5Os",
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1MNqqHMBgaCVRip_4VYQ2U1h3iZ7hewoq",
     "timestamp": 1763489425122
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
