# %% Environment

from copy import deepcopy
import matplotlib.pyplot as plt
import matplotlib.ticker as mtick
import matplotlib.dates as mdates
import numpy as np
import os
import pandas as pd
import talib
import xgboost as xgb
import optuna
import random
import subprocess
import warnings
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score
from tqdm import tqdm

# æŠ‘åˆ¶ Optuna çš„è©³ç´°è¼¸å‡ºï¼Œåªé¡¯ç¤ºéŒ¯èª¤èˆ‡çµæœ (Suppress Optuna verbose output)
optuna.logging.set_verbosity(optuna.logging.WARNING)
warnings.filterwarnings('ignore')

# %matplotlib inline

_Path = r'D:\03Programs_Clouds\Google Drive\NSYSU\05Algorithm Trading\Final Report'
os.chdir(_Path)
    
def set_seed(seed=42):
    """Set seeds for reproducibility."""
    random.seed(seed)
    np.random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    
    # If using deep learning later:
    # tf.random.set_seed(seed)
    # torch.manual_seed(seed)
    # torch.cuda.manual_seed(seed)
    # torch.backends.cudnn.deterministic = True

set_seed(42)

# %% Data Access

Stock = pd.read_parquet("TWSE_before2013.parquet")

# %% Data Cleaning

## Keep codes that are exactly 4 digits, all numbers.
Common = Stock[
    Stock['Code'].astype(str).str.fullmatch(r'\d{4}') &        # 4 digits
    ~Stock['Code'].astype(str).str.startswith('00') &          # no ETF
    ~Stock['Code'].astype(str).str.startswith('01') &          # no CBBC
    ~Stock['Code'].astype(str).str.startswith('91')            # no TDR
    ].copy() # åŠ ä¸Š .copy() é¿å… SettingWithCopyWarning

## ç¢ºä¿ Date æ¬„ä½æ˜¯ datetime æ ¼å¼ä¸¦æ’åº (Ensure datetime format and sorting)
if 'Date' in Common.columns:
    Common['Date'] = pd.to_datetime(Common['Date'])
    Common = Common.sort_values(['Code', 'Date']).reset_index(drop=True)

## Check point.
print(f"Unique Codes: {len(Common['Code'].unique())}")

# ==============================================================================
#                 ğŸ‘‡ ä»¥ä¸‹ç‚ºæ ¹æ“šæ‚¨çš„è¦æ±‚ç”Ÿæˆçš„å…¨æ–°ç­–ç•¥æ¶æ§‹ ğŸ‘‡
# ==============================================================================

# %% [1] Configuration & Hyperparameters (Updated)

# äº¤æ˜“æˆæœ¬
TX_COST_RATE = 0.004
SHORT_COST_RATE = 0.002

# å›æ¸¬è¦–çª—
TRAIN_YEARS = 4
TEST_YEARS = 1

# XGBoost åƒæ•¸
XGB_PARAMS = {
    'n_estimators': 200,
    'max_depth': 6,
    'learning_rate': 0.05,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'objective': 'binary:logistic',
    'eval_metric': 'logloss',
    'n_jobs': -1,
    'random_state': 42
}

# æ ¸å¿ƒç­–ç•¥é–€æª»
# æ ¹æ“šå‰›æ‰çš„åˆ†æï¼Œå»ºè­°æé«˜åšå¤šé–€æª»ï¼Œåšç©ºé–€æª»è¨­é«˜æˆ–é™ä½æ¬Šé‡
THRESHOLD_LONG = 0.60  
THRESHOLD_SHORT = 0.70

# æ¬Šé‡è¨ˆç®—æ–¹å¼
WEIGHTING_METHOD = 'constant' 
OPTUNA_TRIALS = 100

# === Groupmate B å¼·åˆ¶å‡ºå ´è¦å‰‡ ===
STOP_LOSS_RATE = 0.15      
TRAILING_DD_RATE = 0.15    
MAX_HOLD_DAYS = 80         

# === ğŸ”¥ NEW: Short Selling Factor ğŸ”¥ ===
# æ”¾ç©ºèª¿ç¯€ä¿‚æ•¸ï¼š
# 1.0 = æ­£å¸¸åšç©º (100% æ¬Šé‡)
# 0.5 = åšç©ºæ¸›åŠ (50% æ¬Šé‡)
# 0.0 = ç¦æ­¢åšç©º
SHORT_FACTOR = 0.001


# %% [2] Feature Engineering

def calculate_market_features(df_all):
    """è¨ˆç®—å…¨å¸‚å ´ç‰¹å¾µ (å« Market Index, MA200 Bias, Breadth, Volatility)"""
    df = df_all.copy()

    # 1. è¨ˆç®—å€‹åˆ¥è‚¡ç¥¨çš„å­£ç·šç‹€æ…‹ (ç”¨æ–¼å¸‚å ´å»£åº¦)
    df['MA60'] = df.groupby('Code')['Close'].transform(lambda x: x.rolling(60).mean())
    df['Above_MA60'] = (df['Close'] > df['MA60']).astype(int)

    # 2. èšåˆè¨ˆç®—å…¨å¸‚å ´æŒ‡æ¨™
    # æ³¨æ„ï¼šé€™è£¡ä½¿ç”¨ 'mean' ä½œç‚ºç­‰æ¬Šé‡æŒ‡æ•¸çš„ä»£ç†
    mkt = df.groupby('Date').agg({
        'Code': 'count',
        'Above_MA60': 'mean',     # å¸‚å ´å»£åº¦ (Market Breadth)
        'Close': ['std', 'mean'], # å¸‚å ´é›¢æ•£åº¦èˆ‡å‡åƒ¹ (Market Index Proxy)
        'Volume': 'sum'
    })

    # 3. æ¬„ä½é‡æ–°å‘½å
    mkt.columns = ['_'.join(col).strip() for col in mkt.columns.values]
    mkt = mkt.rename(columns={
        'Above_MA60_mean': 'Market_Breadth',
        'Close_std':        'Market_Dispersion',
        'Close_mean':       'Market_Index',      # ä»¥å…¨å¸‚å ´å¹³å‡æ”¶ç›¤åƒ¹ä½œç‚ºå¤§ç›¤æŒ‡æ•¸
        'Volume_sum':       'Market_Volume'
    })

    # 4. è¨ˆç®—å¤§ç›¤å¹´ç·šèˆ‡ä¹–é›¢ç‡ (Market Bias) - çµ¦ AI åˆ¤æ–·ç‰›ç†Šå¸‚çš„é‡è¦ç‰¹å¾µ
    # æ­£å€¼ = ç‰›å¸‚ (åšå¤šæœ‰åˆ©), è² å€¼ = ç†Šå¸‚ (åšç©ºæœ‰åˆ©)
    mkt['Market_MA200'] = mkt['Market_Index'].rolling(200).mean()
    mkt['Feat_Market_Bias_200'] = (mkt['Market_Index'] / mkt['Market_MA200']) - 1

    # 5. è¨ˆç®—å¸‚å ´æˆäº¤é‡è®Šç•° (Volume Delta)
    mkt['Market_Vol_MA5'] = mkt['Market_Volume'].rolling(5).mean()
    mkt['Market_Vol_Delta'] = mkt['Market_Volume'] / mkt['Market_Vol_MA5'].replace(0, np.nan)

    # 6. è¨ˆç®—å¸‚å ´æ³¢å‹•ç‡æŒ‡æ•¸ (Volatility Index)
    mkt['Market_Volatility_Idx'] = mkt['Market_Dispersion'] / mkt['Market_Index']

    # 7. è™•ç†å¯èƒ½çš„ NaN (ä¾‹å¦‚å‰ 200 å¤©æ²’æœ‰ MA200)
    # ä½¿ç”¨ bfill æˆ–å¡« 0 é¿å…è¨“ç·´å‡ºéŒ¯ï¼Œä½†è¦å°å¿ƒå‰æ®µè³‡æ–™åå·®
    mkt = mkt.fillna(0)

    # å›å‚³éœ€è¦çš„ç‰¹å¾µæ¬„ä½
    return mkt[[
        'Market_Breadth', 
        'Market_Dispersion', 
        'Market_Vol_Delta', 
        'Market_Volatility_Idx',
        'Market_Index', 
        'Feat_Market_Bias_200' # æ–°å¢çš„é—œéµç‰¹å¾µ
    ]].reset_index()

def calculate_individual_features(df_stock):
    """è¨ˆç®—å€‹è‚¡ç‰¹å¾µ (å« Groupmate A çš„å‹•èƒ½èˆ‡ä¹–é›¢)"""
    df = df_stock.copy()
    
    # 1. åŸºç¤æŠ€è¡“ç‰¹å¾µ
    df['ATR14'] = talib.ATR(df['High'], df['Low'], df['Close'], timeperiod=14)
    df['ATR50'] = talib.ATR(df['High'], df['Low'], df['Close'], timeperiod=50)
    df['Feat_ATR_Ratio'] = df['ATR14'] / df['ATR50']
    
    u, m, l = talib.BBANDS(df['Close'], timeperiod=20, nbdevup=2.0, nbdevdn=2.0)
    df['Feat_BB_Width'] = (u - l) / m.replace(0, np.nan)
    
    df['Vol_MA20'] = df['Volume'].rolling(20).mean()
    df['Feat_Rel_Vol'] = df['Volume'] / df['Vol_MA20'].replace(0, np.nan)
    df['Feat_RSI_Strength'] = talib.RSI(df['Close'], timeperiod=14)
    
    # 2. Groupmate A çš„ç‰¹å¾µ (Momentum & Bias)
    df['Feat_Ret_1d'] = df['Close'].pct_change(1)
    df['Feat_Ret_5d'] = df['Close'].pct_change(5)
    df['Feat_Ret_20d'] = df['Close'].pct_change(20)
    df['Feat_Vol_20d'] = df['Feat_Ret_1d'].rolling(20).std()
    
    # ä¹–é›¢ç‡
    df['MA5'] = df['Close'].rolling(5).mean()
    df['MA20'] = df['Close'].rolling(20).mean()
    df['MA60'] = df['Close'].rolling(60).mean()
    
    df['Feat_Bias_5'] = (df['Close'] / df['MA5'].replace(0, np.nan)) - 1
    df['Feat_Bias_20'] = (df['Close'] / df['MA20'].replace(0, np.nan)) - 1
    df['Feat_Bias_60'] = (df['Close'] / df['MA60'].replace(0, np.nan)) - 1
    
    # çŸ­æœŸé‡èƒ½
    df['Vol_MA5'] = df['Volume'].rolling(5).mean()
    df['Feat_Vol_Ratio_5'] = df['Volume'] / df['Vol_MA5'].replace(0, np.nan)
    df['Feat_Vol_Change'] = df['Volume'].pct_change()
    
    # æ¸…æ´— Inf/NaN
    df = df.replace([np.inf, -np.inf], np.nan).ffill().fillna(0)
    return df

# %% [3] Strategy Logic Class (Enhanced)

class StrategyLogic:
    @staticmethod
    def strategy_rsi_atr(df, rsi_period, rsi_buy, rsi_sell, atr_period, atr_mult):
        rsi = talib.RSI(df['Close'], timeperiod=int(rsi_period))
        atr = talib.ATR(df['High'], df['Low'], df['Close'], timeperiod=int(atr_period))
        
        signals = np.zeros(len(df))
        position = 0; stop_price = 0.0
        
        for i in range(1, len(df)):
            price = df['Close'].iloc[i]
            if np.isnan(rsi.iloc[i]) or np.isnan(atr.iloc[i]): continue
            
            # Exit Logic (ATR Trailing Stop)
            if position == 1:
                new_stop = price - (atr_mult * atr.iloc[i])
                stop_price = max(stop_price, new_stop)
                if price < stop_price:
                    position = 0; signals[i] = 2
            elif position == -1:
                new_stop = price + (atr_mult * atr.iloc[i])
                stop_price = min(stop_price, new_stop) if stop_price > 0 else new_stop
                if price > stop_price:
                    position = 0; signals[i] = 2
            
            # Entry Logic
            elif position == 0:
                if rsi.iloc[i] < rsi_buy:
                    position = 1; signals[i] = 1; stop_price = price - (atr_mult * atr.iloc[i])
                elif rsi.iloc[i] > rsi_sell:
                    position = -1; signals[i] = -1; stop_price = price + (atr_mult * atr.iloc[i])
        return signals

    @staticmethod
    def strategy_dma_adx(df, fast_ma, slow_ma, adx_period, adx_threshold):
        ma_fast = talib.SMA(df['Close'], timeperiod=int(fast_ma))
        ma_slow = talib.SMA(df['Close'], timeperiod=int(slow_ma))
        adx = talib.ADX(df['High'], df['Low'], df['Close'], timeperiod=int(adx_period))
        
        signals = np.zeros(len(df))
        position = 0
        
        for i in range(1, len(df)):
            if np.isnan(ma_slow.iloc[i]) or np.isnan(adx.iloc[i]): continue
            
            # Exit Logic (Crossover Reverse)
            if position == 1:
                if ma_fast.iloc[i] < ma_slow.iloc[i]:
                    position = 0; signals[i] = 2
            elif position == -1:
                if ma_fast.iloc[i] > ma_slow.iloc[i]:
                    position = 0; signals[i] = 2
            
            # Entry Logic (Trend Strength)
            elif position == 0:
                if adx.iloc[i] > adx_threshold:
                    if ma_fast.iloc[i] > ma_slow.iloc[i] and ma_fast.iloc[i-1] <= ma_slow.iloc[i-1]:
                        position = 1; signals[i] = 1
                    elif ma_fast.iloc[i] < ma_slow.iloc[i] and ma_fast.iloc[i-1] >= ma_slow.iloc[i-1]:
                        position = -1; signals[i] = -1
        return signals

    @staticmethod
    def strategy_macd(df, fast_period, slow_period, signal_period):
        """
        MACD Advanced: åŒ…å« Slope èˆ‡ Divergence (èƒŒé›¢) é‚è¼¯
        """
        macd, signal_line, hist = talib.MACD(df['Close'], 
                                           fastperiod=int(fast_period), 
                                           slowperiod=int(slow_period), 
                                           signalperiod=int(signal_period))
        
        # 1. Slope (å‹•èƒ½è®ŠåŒ–)
        hist_slope = hist - hist.shift(3)
        
        # 2. Divergence (èƒŒé›¢)
        lookback = 5
        price_low = df['Close'] < df['Close'].shift(lookback)
        hist_higher = hist > hist.shift(lookback)
        bull_div = price_low & hist_higher # åº•èƒŒé›¢
        
        price_high = df['Close'] > df['Close'].shift(lookback)
        hist_lower = hist < hist.shift(lookback)
        bear_div = price_high & hist_lower # é ‚èƒŒé›¢
        
        signals = np.zeros(len(df))
        position = 0
        
        hist_arr = hist.values; slope_arr = hist_slope.values
        bull_div_arr = bull_div.values; bear_div_arr = bear_div.values
        
        for i in range(lookback, len(df)):
            if np.isnan(hist_arr[i]): continue
            
            # Exit Logic (Zero Line)
            if position == 1:
                if hist_arr[i] < 0: position = 0; signals[i] = 2
            elif position == -1:
                if hist_arr[i] > 0: position = 0; signals[i] = 2
            
            # Entry Logic
            elif position == 0:
                # Long: (Gold Cross + Slope Up) OR Bull Div
                is_gc = (hist_arr[i] > 0) and (hist_arr[i-1] <= 0)
                if (is_gc and slope_arr[i] > 0) or (bull_div_arr[i] and hist_arr[i] < 0):
                    position = 1; signals[i] = 1
                
                # Short: (Dead Cross + Slope Down) OR Bear Div
                is_dc = (hist_arr[i] < 0) and (hist_arr[i-1] >= 0)
                if (is_dc and slope_arr[i] < 0) or (bear_div_arr[i] and hist_arr[i] > 0):
                    position = -1; signals[i] = -1
        return signals

# %% [4] Helper Functions

def calculate_net_pnl(entry_price, exit_price, position_type):
    """è¨ˆç®—å–®ç­†äº¤æ˜“çš„æ·¨æç›Š (æ‰£é™¤æˆæœ¬)"""
    if position_type == 1: # Long
        return (exit_price - entry_price) - (exit_price * TX_COST_RATE)
    elif position_type == -1: # Short
        return (entry_price - exit_price) - (entry_price * SHORT_COST_RATE) - (exit_price * TX_COST_RATE)
    return 0.0

def get_strategy_pnl(df, signals):
    """è¨ˆç®—ç­–ç•¥åŸå§‹æç›Š (ç”¨æ–¼ Optuna å„ªåŒ–)"""
    total = 0.0; entry = 0.0; pos = 0
    prices = df['Close'].values
    for i in range(len(signals)):
        if signals[i] in [1, -1]:
            entry = prices[i]; pos = int(signals[i])
        elif signals[i] == 2 and pos != 0:
            total += calculate_net_pnl(entry, prices[i], pos)
            pos = 0; entry = 0.0
    return total

# Optuna Objectives
def obj_rsi(t, df):
    p_rsi = t.suggest_int('rsi_p', 10, 25); p_buy = t.suggest_int('rsi_b', 20, 35)
    p_sell = t.suggest_int('rsi_s', 65, 80); p_atr = t.suggest_float('atr_m', 2.0, 4.0, step=0.1)
    return get_strategy_pnl(df, StrategyLogic.strategy_rsi_atr(df, p_rsi, p_buy, p_sell, 14, p_atr))

def obj_dma(t, df):
    p_f = t.suggest_int('f_ma', 5, 20); p_s = t.suggest_int('s_ma', 21, 60)
    p_adx = t.suggest_int('adx_p', 10, 20); p_th = t.suggest_int('adx_t', 15, 30)
    return get_strategy_pnl(df, StrategyLogic.strategy_dma_adx(df, p_f, p_s, p_adx, p_th))

def obj_macd(t, df):
    p_f = t.suggest_int('f_p', 10, 15); p_s = t.suggest_int('s_p', 20, 30); p_sig = t.suggest_int('sig_p', 5, 10)
    return get_strategy_pnl(df, StrategyLogic.strategy_macd(df, p_f, p_s, p_sig))

# XGB Data Preparation (With Peer Signals)
def prepare_xgb_data(df, signals, feature_cols, strat_id, sig_rsi, sig_dma, sig_macd):
    X, y = [], []
    prices = df['Close'].values
    feat_data = df[feature_cols].values
    
    # Peer Signals Arrays (åŒå„•è¨Šè™Ÿ)
    s_rsi = sig_rsi; s_dma = sig_dma; s_macd = sig_macd
    
    entry_idx = -1; entry_price = 0.0; pos = 0
    
    for i in range(len(signals)):
        sig = signals[i]
        if sig in [1, -1]:
            if entry_idx == -1:
                entry_idx = i; entry_price = prices[i]; pos = int(sig)
        elif sig == 2 and entry_idx != -1:
            pnl = calculate_net_pnl(entry_price, prices[i], pos)
            
            # Features: [Base Features] + [Strategy ID] + [Peer Signals]
            feats = list(feat_data[entry_idx])
            feats.append(strat_id)
            feats.append(s_rsi[entry_idx])
            feats.append(s_dma[entry_idx])
            feats.append(s_macd[entry_idx])
            
            X.append(feats)
            y.append(1 if pnl > 0 else 0)
            entry_idx = -1; pos = 0
            
    return np.array(X), np.array(y)

# %% [7] Main Rolling Framework (Updated with SHORT_FACTOR)

def run_framework(common_df):
    print(f"Weighting Method: {WEIGHTING_METHOD}")
    print(f"Short Factor: {SHORT_FACTOR} (Short Sizing Scaler)")
    print(f"Forced Exit Rules: Stop {STOP_LOSS_RATE*100}%, Trail {TRAILING_DD_RATE*100}%, Time {MAX_HOLD_DAYS} days")
    
    print("Step 1: Calculating Market Features...")
    mkt_feats = calculate_market_features(common_df)
    common_df = common_df.merge(mkt_feats, on='Date', how='left')
    
    print("Step 2: Calculating Individual Features...")
    def _apply_feat(g): return calculate_individual_features(g)
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        df_final = common_df.groupby('Code', group_keys=False).apply(_apply_feat)
    
    # ç¢ºä¿åŠ å…¥ Market Bias ç‰¹å¾µ
    base_cols = [
        'Feat_Market_Bias_200', 'Market_Breadth', 
        'Feat_ATR_Ratio', 'Feat_BB_Width', 'Feat_RSI_Strength', 'Feat_Rel_Vol',
        'Feat_Ret_1d', 'Feat_Ret_5d', 'Feat_Ret_20d', 'Feat_Vol_20d',
        'Feat_Bias_5', 'Feat_Bias_20', 'Feat_Bias_60', 'Feat_Vol_Ratio_5', 'Feat_Vol_Change'
    ]
    
    years = sorted(df_final['Date'].dt.year.unique())
    print(f"Data Range: {years[0]} - {years[-1]}")
    
    df_Exe_list = []
    
    for i in range(len(years) - TRAIN_YEARS):
        train_years = years[i : i + TRAIN_YEARS]
        test_year = years[i + TRAIN_YEARS]
        
        print(f"\n=== Window: Train {train_years} | Test {test_year} ===")
        df_train_full = df_final[df_final['Date'].dt.year.isin(train_years)].copy()
        
        # --- A. Training (çœç•¥ç´°ç¯€ï¼Œèˆ‡å‰ç‰ˆç›¸åŒ) ---
        X_train_list, y_train_list = [], []
        # (æ­¤è™•ä¿ç•™åŸæœ¬çš„ Optuna èˆ‡ è¨“ç·´è³‡æ–™ç”Ÿæˆè¿´åœˆ...)
        # ç‚ºç¯€çœç¯‡å¹…ï¼Œå‡è¨­è¨“ç·´è³‡æ–™å·²æ­£ç¢ºç”Ÿæˆ
        # è«‹ç¢ºä¿é€™è£¡ä½¿ç”¨åŸæœ¬å®Œæ•´çš„è¨“ç·´ä»£ç¢¼
        for t in range(len(train_years) - 1):
            opt_yr = train_years[t]; gen_yr = train_years[t+1]
            df_opt = df_train_full[df_train_full['Date'].dt.year == opt_yr]
            df_gen = df_train_full[df_train_full['Date'].dt.year == gen_yr]
            top_stocks = df_opt.groupby('Code')['Volume'].mean().nlargest(10).index.tolist()
            df_opt_s = df_opt[df_opt['Code'].isin(top_stocks)]
            
            s_rsi = optuna.create_study(direction='maximize'); s_rsi.optimize(lambda t: sum([obj_rsi(t, df_opt_s[df_opt_s['Code']==c]) for c in top_stocks]), n_trials=OPTUNA_TRIALS)
            s_dma = optuna.create_study(direction='maximize'); s_dma.optimize(lambda t: sum([obj_dma(t, df_opt_s[df_opt_s['Code']==c]) for c in top_stocks]), n_trials=OPTUNA_TRIALS)
            s_macd = optuna.create_study(direction='maximize'); s_macd.optimize(lambda t: sum([obj_macd(t, df_opt_s[df_opt_s['Code']==c]) for c in top_stocks]), n_trials=OPTUNA_TRIALS)
            p_rsi = s_rsi.best_params; p_dma = s_dma.best_params; p_macd = s_macd.best_params
            
            gen_stocks = df_gen.groupby('Code')['Volume'].mean().nlargest(50).index.tolist()
            for code in gen_stocks:
                df_s = df_gen[df_gen['Code'] == code]
                if len(df_s) < 50: continue
                sig_r = StrategyLogic.strategy_rsi_atr(df_s, p_rsi['rsi_p'], p_rsi['rsi_b'], p_rsi['rsi_s'], 14, p_rsi['atr_m'])
                sig_d = StrategyLogic.strategy_dma_adx(df_s, p_dma['f_ma'], p_dma['s_ma'], p_dma['adx_p'], p_dma['adx_t'])
                sig_m = StrategyLogic.strategy_macd(df_s, p_macd['f_p'], p_macd['s_p'], p_macd['sig_p'])
                X_r, y_r = prepare_xgb_data(df_s, sig_r, base_cols, 0, sig_r, sig_d, sig_m)
                X_d, y_d = prepare_xgb_data(df_s, sig_d, base_cols, 1, sig_r, sig_d, sig_m)
                X_m, y_m = prepare_xgb_data(df_s, sig_m, base_cols, 2, sig_r, sig_d, sig_m)
                if len(X_r)>0: X_train_list.append(X_r); y_train_list.append(y_r)
                if len(X_d)>0: X_train_list.append(X_d); y_train_list.append(y_d)
                if len(X_m)>0: X_train_list.append(X_m); y_train_list.append(y_m)

        # --- B. Train XGBoost ---
        xgb_model = None
        if len(X_train_list) > 0:
            X_train = np.vstack(X_train_list)
            y_train = np.concatenate(y_train_list)
            if len(np.unique(y_train)) > 1:
                xgb_model = xgb.XGBClassifier(**XGB_PARAMS)
                xgb_model.fit(X_train, y_train)

        # --- C. Optimize Params (åŒä¸Š) ---
        last_train_year = train_years[-1]
        df_last = df_train_full[df_train_full['Date'].dt.year == last_train_year]
        top_stocks_last = df_last.groupby('Code')['Volume'].mean().nlargest(10).index.tolist()
        df_last_s = df_last[df_last['Code'].isin(top_stocks_last)]
        
        s_rsi_t = optuna.create_study(direction='maximize'); s_rsi_t.optimize(lambda t: sum([obj_rsi(t, df_last_s[df_last_s['Code']==c]) for c in top_stocks_last]), n_trials=OPTUNA_TRIALS)
        s_dma_t = optuna.create_study(direction='maximize'); s_dma_t.optimize(lambda t: sum([obj_dma(t, df_last_s[df_last_s['Code']==c]) for c in top_stocks_last]), n_trials=OPTUNA_TRIALS)
        s_macd_t = optuna.create_study(direction='maximize'); s_macd_t.optimize(lambda t: sum([obj_macd(t, df_last_s[df_last_s['Code']==c]) for c in top_stocks_last]), n_trials=OPTUNA_TRIALS)
        tp_rsi = s_rsi_t.best_params; tp_dma = s_dma_t.best_params; tp_macd = s_macd_t.best_params

        # --- D. Testing Phase (Updated with SHORT_FACTOR) ---
        print(f"  > Running Test on ALL stocks in {test_year}...")
        df_test = df_final[df_final['Date'].dt.year == test_year].copy()
        test_codes = df_test['Code'].unique()
        
        # è¨“ç·´ç”¨ç‰¹å¾µæ¬„ä½ (æ’é™¤éæ•¸å€¼)
        train_cols = [c for c in base_cols if c in df_test.columns]

        for code in tqdm(test_codes, desc="Testing Stocks"):
            df_s = df_test[df_test['Code'] == code].copy()
            if len(df_s) < 10: continue
            
            s_r = StrategyLogic.strategy_rsi_atr(df_s, tp_rsi['rsi_p'], tp_rsi['rsi_b'], tp_rsi['rsi_s'], 14, tp_rsi['atr_m'])
            s_d = StrategyLogic.strategy_dma_adx(df_s, tp_dma['f_ma'], tp_dma['s_ma'], tp_dma['adx_p'], tp_dma['adx_t'])
            s_m = StrategyLogic.strategy_macd(df_s, tp_macd['f_p'], tp_macd['s_p'], tp_macd['sig_p'])
            
            feat_data = df_s[train_cols].values
            prices = df_s['Close'].values; highs = df_s['High'].values; lows = df_s['Low'].values
            
            curr_pos = 0; entry_price = 0.0; curr_w = 0.0; entry_prob = 0.0; rec_strat_name = "None"
            highest_price = 0.0; lowest_price = 0.0; days_held = 0
            
            exec_sig_list = []; rec_strat_list = []; prob_list = []; weight_list = []; exit_reason_list = []
            net_ret_list = []; gross_ret_list = []; confusion_list = []; conf_w_ret_list = []
            cum_net = 0.0; cum_gross = 0.0
            
            for t in range(len(df_s)):
                day_exec_sig = 0; day_prob = 0.0; day_strat = "None"; day_w = 0.0; day_exit_reason = "None"
                day_net = 0.0; day_gross = 0.0; conf = 0; conf_w_ret = np.nan
                
                # --- A. Check Exit ---
                if curr_pos != 0:
                    should_exit = False; exit_reason = ""; days_held += 1
                    
                    if curr_pos == 1:
                        highest_price = max(highest_price, highs[t])
                        dd = (prices[t] - highest_price) / highest_price
                        unrealized_ret = (prices[t] - entry_price) / entry_price
                    else:
                        lowest_price = min(lowest_price, lows[t]) if lowest_price > 0 else lows[t]
                        dd = (lowest_price - prices[t]) / lowest_price
                        unrealized_ret = (entry_price - prices[t]) / entry_price
                    
                    if unrealized_ret < -STOP_LOSS_RATE: should_exit = True; exit_reason = "HardStop"
                    elif dd < -TRAILING_DD_RATE: should_exit = True; exit_reason = "Trailing"
                    elif days_held >= MAX_HOLD_DAYS: should_exit = True; exit_reason = "Time"
                    else:
                        orig_signal = 0
                        if rec_strat_name == "RSI": orig_signal = s_r[t]
                        elif rec_strat_name == "DMA": orig_signal = s_d[t]
                        elif rec_strat_name == "MACD": orig_signal = s_m[t]
                        if orig_signal == 2: should_exit = True; exit_reason = "Strategy"
                            
                    if should_exit:
                        day_exec_sig = 2; day_exit_reason = exit_reason
                        # PnL è¨ˆç®—ï¼šå› ç‚º entry æ™‚å·²ç¶“æŠŠ curr_w ä¹˜ä»¥äº† SHORT_FACTORï¼Œ
                        # é€™è£¡ç›´æ¥ç”¨ curr_w è¨ˆç®—ï¼Œæç›Šå°±æœƒè‡ªå‹•ç¸®æ”¾ï¼Œç„¡éœ€å†æ¬¡ä¹˜ä¿‚æ•¸ã€‚
                        gross_pnl = (prices[t] - entry_price) * curr_pos * curr_w
                        exit_cost = (prices[t] * curr_w * TX_COST_RATE)
                        net_pnl = gross_pnl - exit_cost
                        day_net += net_pnl; day_gross += gross_pnl
                        
                        if curr_pos == 1: conf = 1 if prices[t] > entry_price else (2 if prices[t] < entry_price else 3)
                        else: conf = -1 if prices[t] < entry_price else (-2 if prices[t] > entry_price else -3)
                        conf_w_ret = entry_prob * net_pnl
                        
                        curr_pos = 0; entry_price = 0.0; curr_w = 0.0; entry_prob = 0.0
                        highest_price = 0.0; lowest_price = 0.0; days_held = 0; rec_strat_name = "None"
                
                # --- B. Check Entry ---
                elif curr_pos == 0 and xgb_model:
                    candidates = []
                    # ç°¡å–®æª¢æŸ¥ç‰¹å¾µé•·åº¦ (é˜²æ­¢ NaN å°è‡´éŒ¯èª¤)
                    try:
                        f_base = list(feat_data[t])
                    except: continue

                    # æ”¶é›†å€™é¸è¨Šè™Ÿ
                    if s_r[t] in [1, -1]:
                        f = f_base + [0, s_r[t], s_d[t], s_m[t]]
                        try: p = xgb_model.predict_proba(np.array([f]))[0][1]
                        except: p = 0
                        candidates.append((p, "RSI", s_r[t]))
                        
                    if s_d[t] in [1, -1]:
                        f = f_base + [1, s_r[t], s_d[t], s_m[t]]
                        try: p = xgb_model.predict_proba(np.array([f]))[0][1]
                        except: p = 0
                        candidates.append((p, "DMA", s_d[t]))

                    if s_m[t] in [1, -1]:
                        f = f_base + [2, s_r[t], s_d[t], s_m[t]]
                        try: p = xgb_model.predict_proba(np.array([f]))[0][1]
                        except: p = 0
                        candidates.append((p, "MACD", s_m[t]))
                        
                    if candidates:
                        candidates.sort(key=lambda x: x[0], reverse=True)
                        winner = candidates[0]
                        best_prob = winner[0]; day_strat = winner[1]; day_exec_sig = winner[2]
                        
                        # æ‡‰ç”¨ä¸åŒçš„é–€æª»
                        thresh = THRESHOLD_LONG if day_exec_sig == 1 else THRESHOLD_SHORT
                        
                        if best_prob > thresh:
                            # è¨ˆç®—åŸºç¤æ¬Šé‡
                            if WEIGHTING_METHOD == 'non-linear': w = ((best_prob - thresh) / (1 - thresh)) ** 2
                            elif WEIGHTING_METHOD == 'linear': w = (best_prob - thresh) / (1 - thresh)
                            else: w = 1.0
                            
                            w = min(w, 1.0)
                            
                            # ğŸ”¥ğŸ”¥ğŸ”¥ é—œéµä¿®æ”¹ï¼šæ‡‰ç”¨ SHORT_FACTOR ğŸ”¥ğŸ”¥ğŸ”¥
                            # å¦‚æœæ˜¯æ”¾ç©ºï¼Œå°‡æ¬Šé‡ä¹˜ä»¥ä¿‚æ•¸ (ä¾‹å¦‚ 0.5)
                            # é€™æœƒç›´æ¥å½±éŸ¿é€²å ´æˆæœ¬ã€å‡ºå ´æˆæœ¬èˆ‡æœ€çµ‚æç›Š
                            if day_exec_sig == -1:
                                w = w * SHORT_FACTOR
                            
                            # åªæœ‰ç•¶èª¿æ•´å¾Œçš„æ¬Šé‡ > 0 æ‰é€²å ´ (è‹¥ FACTOR=0 å‰‡ä¸é€²å ´)
                            if w > 0:
                                day_w = w
                                curr_pos = int(day_exec_sig)
                                entry_price = prices[t]
                                curr_w = day_w
                                entry_prob = best_prob
                                rec_strat_name = day_strat
                                highest_price = prices[t]; lowest_price = prices[t]; days_held = 0
                                
                                entry_cost = (prices[t] * curr_w * TX_COST_RATE) if day_exec_sig==1 else (prices[t] * curr_w * SHORT_COST_RATE)
                                day_net -= entry_cost
                                day_prob = best_prob

                # Update Logs
                cum_net += day_net; cum_gross += day_gross
                exec_sig_list.append(day_exec_sig); rec_strat_list.append(day_strat)
                prob_list.append(day_prob); weight_list.append(day_w); exit_reason_list.append(day_exit_reason)
                net_ret_list.append(cum_net); gross_ret_list.append(cum_gross)
                confusion_list.append(conf); conf_w_ret_list.append(conf_w_ret)
            
            df_s['Rec_Strat'] = rec_strat_list; df_s['Exec_Sig'] = exec_sig_list
            df_s['Prob'] = prob_list; df_s['Weight'] = weight_list
            df_s['Exit_Reason'] = exit_reason_list
            df_s['Net_Cum_Ret'] = net_ret_list; df_s['Gross_Cum_Ret'] = gross_ret_list
            df_s['Confusion'] = confusion_list; df_s['Conf_Weighted_Ret'] = conf_w_ret_list
            df_Exe_list.append(df_s)

    if df_Exe_list:
        df_Exe = pd.concat(df_Exe_list)
        print("\nâœ… df_Exe Generation Complete.")
    else:
        return None, None
    
    # Generate df_Inv
    print("Generating df_Inv...")
    inv_data = []
    for code, g in df_Exe.groupby('Code'):
        tp = (g['Confusion'] == 1).sum()
        fp = (g['Confusion'] == 2).sum()
        tn = (g['Confusion'] == -1).sum()
        fn = (g['Confusion'] == -2).sum()
        total_trades = tp + fp + tn + fn
        
        final_net = g['Net_Cum_Ret'].iloc[-1]
        final_gross = g['Gross_Cum_Ret'].iloc[-1]
        
        equity = 1 + g['Net_Cum_Ret']
        mdd = ((equity - equity.cummax()) / equity.cummax()).min()
        
        daily_pnl = g['Net_Cum_Ret'].diff().fillna(0)
        std = daily_pnl.std()
        sharpe = (daily_pnl.mean() / std * np.sqrt(252)) if std != 0 else 0
            
        inv_data.append({
            'Code': code, 'Gross_Ret': final_gross, 'Net_Ret': final_net,
            'MDD': mdd, 'Sharpe': sharpe, 'Total_Trades': total_trades,
            'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn
        })
    df_Inv = pd.DataFrame(inv_data)
    print("âœ… df_Inv Generation Complete.")
    
    return df_Exe, df_Inv


# %% [6] Execution Entry Point

if __name__ == "__main__":
    if 'Common' in locals() and not Common.empty:
        try:
            print("ğŸš€ Starting Backtest Framework...")
            df_Exe, df_Inv = run_framework(Common)
            
            if df_Inv is not None:
                print("\n" + "="*40)
                print("FINAL REPORT SUMMARY")
                print("="*40)
                
                print("\n--- df_Inv Head (Top 5) ---")
                print(df_Inv.head())
                
                print("\n--- Overall Performance Averages ---")
                print(df_Inv.mean(numeric_only=True))
                
                # è¼¸å‡º CSV
                df_Exe.to_parquet("Transaction_Ledger_df_Exe.parquet", index=False)
                df_Inv.to_csv("Strategy_Performance_df_Inv.csv", index=False)
                print("\nğŸ’¾ Files saved: Transaction_Ledger_df_Exe.csv, Strategy_Performance_df_Inv.csv")
            else:
                print("Result is empty.")
            
        except Exception as e:
            print(f"\nâŒ Error: {e}")
            import traceback
            traceback.print_exc()
    else:
        print("Error: Data 'Common' is not available.")
        

# 2. Calculate Annualized Return with safeguards
def calculate_cagr(net_ret):
    # Case A: Bankruptcy (Loss > 100%)
    if net_ret <= -1.0:
        return (1 + net_ret) ** int(365 / _dayOfTrade) - 1
    
    # Case B: Standard Calculation
    else:
        return (1 + net_ret) ** (365 / _dayOfTrade) - 1

_dayOfTrade = (df_Exe['Date'].max() - df_Exe['Date'].min()).days
df_Inv['A.Net_Ret'] = df_Inv['Net_Ret'].apply(calculate_cagr)

(1 + 7.5659) ** (365 / _dayOfTrade) - 1
7.5659 * 746

# %% [8] Match with Industry

print("ğŸš€ æ­£åœ¨å¾è­‰äº¤æ‰€ (TWSE) ç¶²ç«™æŠ“å–æœ€æ–°çš„å®Œæ•´ç”¢æ¥­æ¸…å–®...")

def fetch_twse_data(url, market_name):
    try:
        # è®€å–ç¶²é è¡¨æ ¼
        dfs = pd.read_html(url)
        df = dfs[0]
        
        # è¨­å®šæ¨™é¡Œåˆ— (é€šå¸¸ç¬¬ä¸€åˆ—æ˜¯æ¨™é¡Œ)
        df.columns = df.iloc[0]
        df = df.iloc[1:]
        
        # æ‰¾åˆ° "æœ‰åƒ¹è­‰åˆ¸ä»£è™ŸåŠåç¨±" é€™ä¸€æ¬„
        col_name = [c for c in df.columns if 'æœ‰åƒ¹è­‰åˆ¸ä»£è™ŸåŠåç¨±' in str(c)][0]
        
        # æ‹†åˆ†ä»£è™Ÿèˆ‡åç¨± (æ ¼å¼é€šå¸¸æ˜¯ "1101ã€€å°æ³¥")
        # ä½¿ç”¨ split æ‹†é–‹ï¼Œç¬¬ä¸€å€‹æ˜¯ä»£è™Ÿï¼Œç¬¬äºŒå€‹æ˜¯åç¨±
        df['Code'] = df[col_name].astype(str).str.split(n=1).str[0]
        df['Name'] = df[col_name].astype(str).str.split(n=1).str[1]
        
        # å°æ‡‰ç”¢æ¥­æ¬„ä½
        if 'ç”¢æ¥­åˆ¥' in df.columns:
            df['Industry'] = df['ç”¢æ¥­åˆ¥']
        else:
            df['Industry'] = 'Unknown'
            
        print(f"âœ… {market_name} è³‡æ–™ä¸‹è¼‰æˆåŠŸ: {len(df)} ç­†")
        return df[['Code', 'Name', 'Industry']]
        
    except Exception as e:
        print(f"âŒ {market_name} è³‡æ–™ä¸‹è¼‰å¤±æ•—: {e}")
        return pd.DataFrame()

# 1. ä¸‹è¼‰ ä¸Šå¸‚ (Mode=2) èˆ‡ ä¸Šæ«ƒ (Mode=4)
df_listed = fetch_twse_data("https://isin.twse.com.tw/isin/C_public.jsp?strMode=2", "ä¸Šå¸‚")
df_otc    = fetch_twse_data("https://isin.twse.com.tw/isin/C_public.jsp?strMode=4", "ä¸Šæ«ƒ")

# 2. åˆä½µå…©è€…
df_industry_web = pd.concat([df_listed, df_otc], ignore_index=True)

# 3. è³‡æ–™æ¸…ç†
# éæ¿¾æ‰éè‚¡ç¥¨çš„ä»£è™Ÿ (æœ‰äº›æ˜¯ warrants æˆ– ETFï¼Œé•·åº¦ä¸å°æˆ–é–‹é ­ç‰¹æ®Š)
# é€™è£¡ç°¡å–®éæ¿¾ï¼šåªç•™ä»£è™Ÿé•·åº¦ç‚º 4 çš„ (æ™®é€šè‚¡)
df_industry_web = df_industry_web[df_industry_web['Code'].str.len() == 4]

# å¼·åˆ¶æ¸…ç†æ ¼å¼
df_industry_web['Code'] = df_industry_web['Code'].astype(str).str.strip()
df_Inv['Code'] = df_Inv['Code'].astype(str).str.strip()

# 4. åˆä½µé€² df_Inv
# ç§»é™¤èˆŠæ¬„ä½
for col in ['Name', 'Industry']:
    if col in df_Inv.columns:
        df_Inv = df_Inv.drop(columns=[col])

# åˆä½µ
df_Inv = pd.merge(df_Inv, df_industry_web, on='Code', how='left')

# å¡«è£œç©ºå€¼
df_Inv['Name'] = df_Inv['Name'].fillna('Unknown')
df_Inv['Industry'] = df_Inv['Industry'].fillna('Unknown')

print("\nğŸ“Š æœ€çµ‚åˆä½µçµæœ (å‰ 5 ç­†):")
print(df_Inv[['Code', 'Name', 'Industry', 'Net_Ret']].head())

# æª¢æŸ¥ 1101 æ˜¯å¦æˆåŠŸ
print("\nğŸ” æª¢æŸ¥ 1101 (å°æ³¥):")
print(df_Inv[df_Inv['Code'] == '1101'][['Code', 'Name', 'Industry']])


        
# %% [9] Analysis: Profitability by Probability Bucket (Fixed Version)

def analyze_prob_performance_fixed(df_exe):
    print("\n" + "="*60)
    print("ğŸ“Š Analysis: Net Return by ENTRY Probability (Fixed)")
    print("="*60)
    
    df = df_exe.copy()
    
    # 1. ä¿®æ­£ï¼šå›æº¯ã€Œé€²å ´æ™‚ã€çš„æ©Ÿç‡ (Entry Probability)
    # é‚è¼¯ï¼šå»ºç«‹ä¸€å€‹æ–°æ¬„ä½ï¼Œåªåœ¨é€²å ´æ—¥å¡«å…¥ Probï¼Œç„¶å¾Œå‘ä¸‹å¡«å…… (Forward Fill) åˆ°å‡ºå ´æ—¥
    df['Entry_Prob_Fixed'] = np.nan
    
    # æ¨™è¨˜é€²å ´é» (Exec_Sig ç‚º 1 æˆ– -1)
    entry_mask = df['Exec_Sig'].isin([1, -1])
    df.loc[entry_mask, 'Entry_Prob_Fixed'] = df.loc[entry_mask, 'Prob']
    
    # é‡å°æ¯ä¸€æª”è‚¡ç¥¨é€²è¡Œ Forward Fill
    # é€™æ¨£å‡ºå ´æ—¥ (Exec_Sig=2) å°±æœƒæ‹¿åˆ°æœ€è¿‘ä¸€æ¬¡é€²å ´æ—¥çš„æ©Ÿç‡
    df['Entry_Prob_Fixed'] = df.groupby('Code')['Entry_Prob_Fixed'].ffill()
    
    # 2. éæ¿¾ï¼šåªä¿ç•™æœ‰çµç®—çš„å‡ºå ´æ—¥ (Conf_Weighted_Ret éç©º)
    df_res = df.dropna(subset=['Conf_Weighted_Ret']).copy()
    
    if df_res.empty:
        print("No closed trades to analyze.")
        return

    # 3. å»ºç«‹ 1% çš„å€é–“ (Bucket) ä½¿ç”¨ä¿®æ­£å¾Œçš„é€²å ´æ©Ÿç‡
    df_res['Prob_Bucket'] = np.floor(df_res['Entry_Prob_Fixed'] * 100) / 100
    
    # 4. åˆ†çµ„çµ±è¨ˆ
    bucket_stats = df_res.groupby('Prob_Bucket').agg({
        'Conf_Weighted_Ret': 'sum',          
        'Net_Cum_Ret': 'count',              
        'Confusion': lambda x: (x.abs()==1).sum() / x.count() 
    }).rename(columns={'Net_Cum_Ret': 'Trade_Count', 'Confusion': 'Win_Rate'})
    
    # 5. è¼¸å‡ºå ±è¡¨
    bucket_stats = bucket_stats.sort_index(ascending=True)
    
    print(f"{'Entry Prob':<15} | {'Trades':>8} | {'Total Conf_W_Ret':>18} | {'Win Rate':>10}")
    print("-" * 65)
    
    for prob, row in bucket_stats.iterrows():
        range_str = f"{prob:.2f} - {prob+0.01:.2f}"
        print(f"{range_str:<15} | {int(row['Trade_Count']):>8} | {row['Conf_Weighted_Ret']:>18.6f} | {row['Win_Rate']:>10.1%}")

    # 6. ç¹ªåœ–
    if len(bucket_stats) > 0:
        plt.figure(figsize=(12, 6))
        # é¡è‰²ï¼šç´…(è™§) ç¶ (è³º)
        colors = ['red' if x < 0 else 'green' for x in bucket_stats['Conf_Weighted_Ret']]
        
        plt.bar(bucket_stats.index + 0.005, bucket_stats['Conf_Weighted_Ret'], width=0.008, color=colors, alpha=0.7)
        plt.xlabel('XGB Entry Probability (Confidence)')
        plt.ylabel('Total Conf_Weighted_Ret')
        plt.title('Profitability by Entry Confidence Level (Fixed)')
        plt.grid(True, alpha=0.3)
        plt.show()

# åŸ·è¡Œä¿®æ­£å¾Œçš„åˆ†æ
if 'df_Exe' in locals() and not df_Exe.empty:
    analyze_prob_performance_fixed(df_Exe)
else:
    print("df_Exe not found.")
    
    
    
    
# %% [10] Analysis: Weight Effect Analysis (Double Weighted)

def analyze_weight_effect(df_exe):
    print("\n" + "="*60)
    print("ğŸ“Š Analysis: Weight Effect (Net_PnL * Weight) by Confidence")
    print("   Formula: (Conf_Weighted_Ret / Prob) * Weight")
    print("   Note: This effectively squares the weight impact on PnL.")
    print("="*60)
    
    df = df_exe.copy()
    
    # 1. å›æº¯ã€Œé€²å ´æ™‚ã€çš„æ©Ÿç‡èˆ‡æ¬Šé‡
    # å› ç‚ºå¹³å€‰æ—¥ (Exec_Sig=2) çš„ Weight æ˜¯ 0ï¼Œå¿…é ˆå¾é€²å ´æ—¥å¸¶éä¾†
    df['Entry_Prob_Fixed'] = np.nan
    df['Entry_Weight_Fixed'] = np.nan
    
    # æ¨™è¨˜é€²å ´é»
    entry_mask = df['Exec_Sig'].isin([1, -1])
    df.loc[entry_mask, 'Entry_Prob_Fixed'] = df.loc[entry_mask, 'Prob']
    df.loc[entry_mask, 'Entry_Weight_Fixed'] = df.loc[entry_mask, 'Weight']
    
    # Forward Fill
    df['Entry_Prob_Fixed'] = df.groupby('Code')['Entry_Prob_Fixed'].ffill()
    df['Entry_Weight_Fixed'] = df.groupby('Code')['Entry_Weight_Fixed'].ffill()
    
    # 2. éæ¿¾å‡ºå¹³å€‰äº¤æ˜“
    df_res = df.dropna(subset=['Conf_Weighted_Ret']).copy()
    
    if df_res.empty:
        print("No closed trades to analyze.")
        return

    # 3. è¨ˆç®—ä½¿ç”¨è€…è¦æ±‚çš„æŒ‡æ¨™
    # New Metric = (Conf_Weighted_Ret / Prob) * Weight
    # é€™è£¡çš„ Prob å’Œ Weight éƒ½æ˜¯é€²å ´ç•¶ä¸‹çš„å€¼
    df_res['Weight_Effect_Ret'] = (df_res['Conf_Weighted_Ret'] / df_res['Entry_Prob_Fixed']) * df_res['Entry_Weight_Fixed']
    
    # 4. å»ºç«‹ 1% å€é–“
    df_res['Prob_Bucket'] = np.floor(df_res['Entry_Prob_Fixed'] * 100) / 100
    
    # 5. åˆ†çµ„çµ±è¨ˆ
    bucket_stats = df_res.groupby('Prob_Bucket').agg({
        'Weight_Effect_Ret': 'sum',
        'Net_Cum_Ret': 'count', # äº¤æ˜“æ¬¡æ•¸
        'Confusion': lambda x: (x.abs()==1).sum() / x.count()
    }).rename(columns={'Net_Cum_Ret': 'Trade_Count', 'Confusion': 'Win_Rate'})
    
    # 6. è¼¸å‡ºå ±è¡¨
    bucket_stats = bucket_stats.sort_index(ascending=True)
    
    print(f"{'Entry Prob':<15} | {'Trades':>8} | {'Weight_Effect_Ret':>20} | {'Win Rate':>10}")
    print("-" * 65)
    
    for prob, row in bucket_stats.iterrows():
        range_str = f"{prob:.2f} - {prob+0.01:.2f}"
        print(f"{range_str:<15} | {int(row['Trade_Count']):>8} | {row['Weight_Effect_Ret']:>20.6f} | {row['Win_Rate']:>10.1%}")

    # 7. ç¹ªåœ–
    if len(bucket_stats) > 0:
        plt.figure(figsize=(12, 6))
        colors = ['red' if x < 0 else 'purple' for x in bucket_stats['Weight_Effect_Ret']]
        
        plt.bar(bucket_stats.index + 0.005, bucket_stats['Weight_Effect_Ret'], width=0.008, color=colors, alpha=0.7)
        plt.xlabel('XGB Entry Probability')
        plt.ylabel('Total Weight_Effect_Ret')
        plt.title('Impact of Non-Linear Weighting by Confidence Level')
        plt.grid(True, alpha=0.3)
        plt.show()

# åŸ·è¡Œåˆ†æ
if 'df_Exe' in locals() and not df_Exe.empty:
    analyze_weight_effect(df_Exe)
else:
    print("df_Exe not found.")
    
    
# %% [11] Analysis: Portfolio Cumulative Returns (Equal Weighted)

def plot_portfolio_gross_net_styled(df_exe, total_stocks_count=746):
    print("ğŸš€ ç¹ªè£½æŠ•è³‡çµ„åˆåœ–è¡¨ (åŠ å¤§è»¸å­—é«” + çµ±ä¸€è‰²å¡Šé¢¨æ ¼)...")
    
    if df_exe is None or df_exe.empty:
        print("Error: df_Exe is empty.")
        return

    # 1. è³‡æ–™è™•ç†
    df = df_exe[['Date', 'Code', 'Net_Cum_Ret', 'Gross_Cum_Ret', 'Close']].copy()
    df['Date'] = pd.to_datetime(df['Date'])
    
    df = df.sort_values(['Code', 'Date'])
    
    # Net Processing
    df['Daily_Net_PnL'] = df.groupby('Code')['Net_Cum_Ret'].diff().fillna(0)
    mask_net = (df['Net_Cum_Ret'] != 0) & (df['Daily_Net_PnL'] == 0)
    df.loc[mask_net, 'Daily_Net_PnL'] = df.loc[mask_net, 'Net_Cum_Ret']
    
    # Gross Processing
    df['Daily_Gross_PnL'] = df.groupby('Code')['Gross_Cum_Ret'].diff().fillna(0)
    mask_gross = (df['Gross_Cum_Ret'] != 0) & (df['Daily_Gross_PnL'] == 0)
    df.loc[mask_gross, 'Daily_Gross_PnL'] = df.loc[mask_gross, 'Gross_Cum_Ret']
    
    # Convert to Percentage Contribution
    df['Daily_Net_Contrib'] = df['Daily_Net_PnL'] / df['Close']
    df['Daily_Gross_Contrib'] = df['Daily_Gross_PnL'] / df['Close']
    
    # Aggregate
    daily_stats = df.groupby('Date')[['Daily_Net_Contrib', 'Daily_Gross_Contrib']].sum()
    daily_stats /= total_stocks_count
    cum_stats = daily_stats.cumsum()
    
    # 2. æ‰¾å‡ºé—œéµé»
    net_min_val = cum_stats['Daily_Net_Contrib'].min()
    net_min_date = cum_stats['Daily_Net_Contrib'].idxmin()
    gross_min_val = cum_stats['Daily_Gross_Contrib'].min()
    gross_min_date = cum_stats['Daily_Gross_Contrib'].idxmin()
    
    net_final_val = cum_stats['Daily_Net_Contrib'].iloc[-1]
    gross_final_val = cum_stats['Daily_Gross_Contrib'].iloc[-1]
    last_date = cum_stats.index[-1]

    # --- ç¹ªåœ– ---
    plt.figure(figsize=(14, 8))
    
    # ç•«ç·š
    plt.plot(cum_stats.index, cum_stats['Daily_Gross_Contrib'], 
             label='Gross Return', color='blue', linewidth=2, alpha=0.6, linestyle='--')
    
    plt.plot(cum_stats.index, cum_stats['Daily_Net_Contrib'], 
             label='Net Return', color='red', linewidth=2.5)
    
    plt.fill_between(cum_stats.index, cum_stats['Daily_Gross_Contrib'], cum_stats['Daily_Net_Contrib'], 
                     color='gray', alpha=0.1, label='Transaction Costs')
    
    plt.axhline(0, color='black', linestyle='-', linewidth=1, alpha=0.3)

    # æ¨£å¼è¨­å®š
    style_net = dict(boxstyle="round,pad=0.3", fc="red", ec="darkred", alpha=0.9)
    style_gross = dict(boxstyle="round,pad=0.3", fc="blue", ec="navy", alpha=0.8)

    # æ¨™è¨˜æœ€ä½é»
    plt.scatter(net_min_date, net_min_val, color='black', s=100, marker='v', zorder=10)
    plt.annotate(f"Lowest Net: {net_min_val:.2%}", 
                 xy=(net_min_date, net_min_val), 
                 xytext=(10, -20), textcoords='offset points', 
                 arrowprops=dict(arrowstyle="->", color='black'),
                 fontsize=12, fontweight='bold', color='white',
                 bbox=style_net)

    plt.scatter(gross_min_date, gross_min_val, color='black', s=80, marker='v', zorder=10)
    plt.annotate(f"Lowest Gross: {gross_min_val:.2%}", 
                 xy=(gross_min_date, gross_min_val), 
                 xytext=(0, 30), textcoords='offset points', 
                 arrowprops=dict(arrowstyle="->", color='black'),
                 fontsize=12, fontweight='bold', color='white',
                 bbox=style_gross)

    # æ¨™è¨˜æœ€çµ‚å ±é…¬
    plt.annotate(f"Final Net: {net_final_val:.2%}", 
                 xy=(last_date, net_final_val), 
                 xytext=(10, 0), textcoords='offset points',
                 fontsize=12, fontweight='bold', color='white',
                 verticalalignment='center',
                 bbox=style_net)

    plt.annotate(f"Final Gross: {gross_final_val:.2%}", 
                 xy=(last_date, gross_final_val), 
                 xytext=(10, 0), textcoords='offset points',
                 fontsize=12, fontweight='bold', color='white',
                 verticalalignment='center',
                 bbox=style_gross)

    # ==========================================
    # ğŸ”¥ğŸ”¥ğŸ”¥ é—œéµä¿®æ”¹ï¼šåŠ å¤§å­—é«” ğŸ”¥ğŸ”¥ğŸ”¥
    # ==========================================
    
    # 1. åŠ å¤§æ¨™é¡Œ (Title) -> 20
    plt.title(f'Portfolio Cumulative Return (Equal Weight, N={total_stocks_count})', fontsize=20, fontweight='bold', pad=15)
    
    # 2. åŠ å¤§è»¸æ¨™ç±¤ (Labels) -> 16
    plt.xlabel('Date', fontsize=18, fontweight='bold')
    plt.ylabel('Cumulative Return (%)', fontsize=18, fontweight='bold')
    
    # 3. åŠ å¤§è»¸åˆ»åº¦æ•¸å­— (Ticks) -> 14
    plt.tick_params(axis='both', which='major', labelsize=14)
    
    # Yè»¸æ ¼å¼
    plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))
    
    # åŠ å¤§åœ–ä¾‹å­—é«”
    plt.legend(loc='upper left', fontsize=16)
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print(f"âœ… è¨ˆç®—å®Œæˆã€‚")
    
    return net_final_val, gross_final_val

# åŸ·è¡Œ
final_net, final_gross = plot_portfolio_gross_net_styled(df_Exe)
print(f"å¤–éƒ¨æ¥æ”¶åˆ°çš„æ•¸å€¼ -> Net: {final_net}, Gross: {final_gross}")
costs_incurred = final_gross - final_net

_dayOfTrade = (df_Exe['Date'].max() - df_Exe['Date'].min()).days
(1 + final_gross) ** (365 / _dayOfTrade) - 1
(1 + final_net) ** (365 / _dayOfTrade) - 1
(1 + final_gross) ** (365 / _dayOfTrade) - (1 + final_net) ** (365 / _dayOfTrade)


    
# %% [12] Analysis: Plot Cumulative Gross Return for the Median Stock

def plot_median_stock_performance(df_inv, df_exe):
    print("\n" + "="*60)
    print("ğŸ“Š Analysis: Median Performer Deep Dive")
    print("   Finding the stock with median Net_Ret in df_Inv")
    print("="*60)
    
    if df_inv is None or df_inv.empty or df_exe is None or df_exe.empty:
        print("Dataframes are empty. Cannot perform analysis.")
        return

    # 1. æ‰¾å‡ºä¸­ä½æ•¸è‚¡ç¥¨ (Find Median Stock)
    # æ ¹æ“š Net_Ret æ’åº
    df_sorted = df_inv.sort_values(by='Net_Ret').reset_index(drop=True)
    
    # å–å¾—ä¸­ä½æ•¸ç´¢å¼•
    median_idx = len(df_sorted) // 2
    median_stock_info = df_sorted.iloc[median_idx]
    
    target_code = median_stock_info['Code']
    target_net_ret = median_stock_info['Net_Ret']
    target_gross_ret = median_stock_info['Gross_Ret']
    
    print(f"ğŸ¯ Median Stock Found: {target_code}")
    print(f"   Net Return:   {target_net_ret:.4f}")
    print(f"   Gross Return: {target_gross_ret:.4f}")
    print(f"   Total Trades: {median_stock_info['Total_Trades']}")

    # 2. å¾ df_Exe æå–è©²è‚¡ç¥¨çš„è©³ç´°æ•¸æ“š (Extract Data)
    stock_data = df_exe[df_exe['Code'] == target_code].copy()
    
    # ç¢ºä¿æ—¥æœŸæ ¼å¼æ­£ç¢º
    if not np.issubdtype(stock_data['Date'].dtype, np.datetime64):
        stock_data['Date'] = pd.to_datetime(stock_data['Date'])
        
    # 3. ç¹ªåœ– (Plot)
    plt.figure(figsize=(12, 6))
    
    # ç¹ªè£½ Gross Return
    plt.plot(stock_data['Date'], stock_data['Gross_Cum_Ret'], 
             label=f'Gross Cum Ret ({target_code})', color='blue', linewidth=1.5)
    
    # ä¹Ÿå¯ä»¥é †ä¾¿ç•«å‡º Net Return ä¾›æ¯”è¼ƒ (è™›ç·š)
    plt.plot(stock_data['Date'], stock_data['Net_Cum_Ret'], 
             label=f'Net Cum Ret ({target_code})', color='red', linestyle='--', linewidth=1.5, alpha=0.7)
    
    plt.title(f'Performance of Median Stock: {target_code} (Net Ret: {target_net_ret:.2f})')
    plt.xlabel('Date')
    plt.ylabel('Cumulative Return')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.show()

# åŸ·è¡Œåˆ†æ
if 'df_Inv' in locals() and 'df_Exe' in locals():
    plot_median_stock_performance(df_Inv, df_Exe)
else:
    print("df_Inv or df_Exe not found. Please run the main backtest first.")


# %% [Analysis] Re-check Median Stock 1608 Performance
if 'df_Inv' in locals() and 'df_Exe' in locals():
    # æŒ‡å®šçœ‹ 1608 é€™æ”¯è‚¡ç¥¨ (åŸæœ¬çš„ä¸­ä½æ•¸/å¤§è³ è‚¡ç¥¨)
    target_code = '1608' 
    
    stock_data = df_Exe[df_Exe['Code'] == target_code].copy()
    if not stock_data.empty:
        plt.figure(figsize=(12, 6))
        plt.plot(stock_data['Date'], stock_data['Gross_Cum_Ret'], label=f'Gross Cum Ret ({target_code})', color='blue')
        plt.plot(stock_data['Date'], stock_data['Net_Cum_Ret'], label=f'Net Cum Ret ({target_code})', color='red', linestyle='--')
        
        # æ¨™ç¤ºå‡ºå ´é» (å¦‚æœæœ‰)
        exits = stock_data[stock_data['Exec_Sig'] == 2]
        plt.scatter(exits['Date'], exits['Net_Cum_Ret'], color='black', marker='x', s=100, label='Exit')
        
        plt.title(f'Performance Verification: Stock {target_code} (With Forced Exit Rules)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()
        
        print(f"Final Net Return for {target_code}: {stock_data['Net_Cum_Ret'].iloc[-1]:.4f}")
    else:
        print(f"Stock {target_code} not found in results.")


# %% [14] Analysis: Sum Conf_Weighted_Ret by Year

def analyze_yearly_performance(df_exe):
    print("\n" + "="*60)
    print("ğŸ“Š Analysis: Total Conf_Weighted_Ret by Year")
    print("="*60)
    
    if df_exe is None or df_exe.empty:
        print("df_Exe is empty. Cannot perform analysis.")
        return

    # 1. è¤‡è£½è³‡æ–™ä¸¦ç¢ºä¿æ—¥æœŸæ ¼å¼æ­£ç¢º
    df = df_exe.copy()
    if not np.issubdtype(df['Date'].dtype, np.datetime64):
        df['Date'] = pd.to_datetime(df['Date'])
        
    # 2. æå–å¹´ä»½
    df['Year'] = df['Date'].dt.year
    
    # 3. åˆ†çµ„åŠ ç¸½ (Group by Year & Sum)
    yearly_sum = df.groupby('Year')['Conf_Weighted_Ret'].sum()
    
    # 4. è¼¸å‡ºå ±è¡¨
    print(f"{'Year':<6} | {'Total Conf_Weighted_Ret':>25}")
    print("-" * 35)
    for year, value in yearly_sum.items():
        print(f"{year:<6} | {value:>25.6f}")
        
    # 5. ç¹ªåœ– (Bar Chart)
    plt.figure(figsize=(10, 6))
    # æ­£å ±é…¬ç‚ºç¶ è‰²ï¼Œè² å ±é…¬ç‚ºç´…è‰²
    colors = ['red' if v < 0 else 'green' for v in yearly_sum.values]
    bars = plt.bar(yearly_sum.index, yearly_sum.values, color=colors, alpha=0.7)
    
    plt.title('Total Confidence-Weighted Return by Year')
    plt.xlabel('Year')
    plt.ylabel('Sum of Conf_Weighted_Ret')
    plt.xticks(yearly_sum.index) # ç¢ºä¿æ¯å¹´éƒ½é¡¯ç¤º
    plt.grid(axis='y', linestyle='--', alpha=0.3)
    
    # åœ¨æŸ±ç‹€åœ–ä¸Šæ¨™ç¤ºæ•¸å€¼
    for bar in bars:
        height = bar.get_height()
        offset = 5 if height >= 0 else -15
        plt.text(bar.get_x() + bar.get_width()/2., height + offset,
                 f'{height:.0f}',
                 ha='center', va='bottom' if height > 0 else 'top', fontsize=9)
                 
    plt.show()

# åŸ·è¡Œåˆ†æ
if 'df_Exe' in locals() and not df_Exe.empty:
    analyze_yearly_performance(df_Exe)
else:
    print("df_Exe not found. Please run the main backtest first.")
    
    
# %% [Analysis] Total Error Rates Analysis (FPR & FNR)

def analyze_total_error_rates(df_inv):
    print("\n" + "="*60)
    print("ğŸ“Š Analysis: Portfolio Total Error Rates (FPR & FNR)")
    print("="*60)
    
    if df_inv is None or df_inv.empty:
        print("df_Inv is empty. Cannot perform analysis.")
        return

    # 1. åŠ ç¸½æ‰€æœ‰å€‹è‚¡çš„æ··æ·†çŸ©é™£æ•¸å€¼
    total_tp = df_inv['TP'].sum() # Long Win (å¸‚å ´æ¼²ï¼Œåšå°äº†)
    total_fp = df_inv['FP'].sum() # Long Loss (å¸‚å ´è·Œï¼ŒåšéŒ¯äº†)
    total_tn = df_inv['TN'].sum() # Short Win (å¸‚å ´è·Œï¼Œåšå°äº†)
    total_fn = df_inv['FN'].sum() # Short Loss (å¸‚å ´æ¼²ï¼ŒåšéŒ¯äº†)
    
    total_trades = total_tp + total_fp + total_tn + total_fn
    
    # 2. è¨ˆç®—æ¯”ç‡
    # FPR: åœ¨æ‰€æœ‰"è©²è·Œ"çš„æ™‚å€™(TN+FP)ï¼Œæˆ‘å€‘èª¤åˆ¤åšå¤š(FP)çš„æ©Ÿç‡
    actual_negatives = total_fp + total_tn
    fpr = total_fp / actual_negatives if actual_negatives > 0 else 0.0
    
    # FNR: åœ¨æ‰€æœ‰"è©²æ¼²"çš„æ™‚å€™(TP+FN)ï¼Œæˆ‘å€‘èª¤åˆ¤åšç©º(FN)çš„æ©Ÿç‡
    actual_positives = total_tp + total_fn
    fnr = total_fn / actual_positives if actual_positives > 0 else 0.0
    
    # Precision (æŸ¥æº–ç‡): åšå¤šæ™‚ï¼ŒçœŸçš„æ¼²çš„æ©Ÿç‡
    precision_long = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0
    
    # Precision (Short): åšç©ºæ™‚ï¼ŒçœŸçš„è·Œçš„æ©Ÿç‡
    precision_short = total_tn / (total_tn + total_fn) if (total_tn + total_fn) > 0 else 0.0

    # 3. è¼¸å‡ºå ±è¡¨
    print(f"{'Metric':<25} | {'Value':<15} | {'Description'}")
    print("-" * 75)
    print(f"{'Total Trades':<25} | {int(total_trades):<15} | Total executed signals")
    print(f"{'Total Long Wins (TP)':<25} | {int(total_tp):<15} | Correctly bought dip/trend")
    print(f"{'Total Long Loss (FP)':<25} | {int(total_fp):<15} | Bought but price fell")
    print(f"{'Total Short Wins (TN)':<25} | {int(total_tn):<15} | Correctly shorted top")
    print(f"{'Total Short Loss (FN)':<25} | {int(total_fn):<15} | Shorted but price rose")
    print("-" * 75)
    
    # é‡é»æŒ‡æ¨™
    print(f"{'False Positive Rate':<25} | {fpr:>14.2%} | Long Error Rate (Bad Longs / All Downtrends)")
    print(f"{'False Negative Rate':<25} | {fnr:>14.2%} | Short Error Rate (Bad Shorts / All Uptrends)")
    print("-" * 75)
    print(f"{'Long Precision':<25} | {precision_long:>14.2%} | Win Rate when Long")
    print(f"{'Short Precision':<25} | {precision_short:>14.2%} | Win Rate when Short")
    print("="*60)
    
    # 4. ç¹ªè£½æ··æ·†çŸ©é™£åœ– (Confusion Matrix Visualization)
    try:
        import seaborn as sns
        
        # å»ºç«‹çŸ©é™£æ•¸æ“š (2x2)
        #           Predicted Long    Predicted Short
        # Actual Up      TP                FN
        # Actual Down    FP                TN
        # æ³¨æ„ï¼šé€™è£¡çš„æ¨™ç±¤æ˜¯ã€Œé æ¸¬æ–¹å‘ã€ï¼Œè¡Œæ˜¯ã€Œå¯¦éš›æ–¹å‘ã€
        
        matrix_data = np.array([[total_tp, total_fn], 
                                [total_fp, total_tn]])
        
        plt.figure(figsize=(8, 6))
        
        # ä½¿ç”¨ç™¾åˆ†æ¯”è¨»é‡‹
        group_names = ['TP (Long Win)', 'FN (Short Loss)', 'FP (Long Loss)', 'TN (Short Win)']
        group_counts = ["{0:0.0f}".format(value) for value in matrix_data.flatten()]
        group_percentages = ["{0:.2%}".format(value) for value in matrix_data.flatten()/np.sum(matrix_data)]
        
        labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in zip(group_names, group_counts, group_percentages)]
        labels = np.asarray(labels).reshape(2,2)
        
        sns.heatmap(matrix_data, annot=labels, fmt='', cmap='Blues', cbar=False,
                    xticklabels=['Pred Long', 'Pred Short'],
                    yticklabels=['Actual Up', 'Actual Down'])
        
        plt.title('Portfolio Total Confusion Matrix')
        plt.ylabel('Actual Market Direction')
        plt.xlabel('Model Action')
        plt.show()
        
    except ImportError:
        print("Seaborn not installed, skipping heatmap.")

# åŸ·è¡Œåˆ†æ
if 'df_Inv' in locals() and not df_Inv.empty:
    analyze_total_error_rates(df_Inv)
else:
    print("df_Inv not found. Please run the backtest framework first.")


# %% [10] Analysis: Profitability by Bucket (Split Long vs Short)

def analyze_long_short_profitability(df_exe):
    print("\n" + "="*60)
    print("ğŸ“Š Analysis: Long vs Short Profitability by Probability")
    print("="*60)
    
    df = df_exe.copy()
    
    # 1. è³‡æ–™å‰è™•ç†ï¼šå›æº¯ã€Œé€²å ´æ™‚ã€çš„ æ©Ÿç‡ èˆ‡ æ–¹å‘
    # å»ºç«‹æ–°æ¬„ä½
    df['Entry_Prob_Fixed'] = np.nan
    df['Entry_Type'] = np.nan # 1: Long, -1: Short
    
    # æ¨™è¨˜é€²å ´é» (Exec_Sig ç‚º 1 æˆ– -1)
    entry_mask = df['Exec_Sig'].isin([1, -1])
    
    # å¡«å…¥é€²å ´ç•¶ä¸‹çš„è³‡è¨Š
    df.loc[entry_mask, 'Entry_Prob_Fixed'] = df.loc[entry_mask, 'Prob']
    df.loc[entry_mask, 'Entry_Type'] = df.loc[entry_mask, 'Exec_Sig']
    
    # Forward Fill: è®“å‡ºå ´æ—¥ (Exec_Sig=2) æ‹¿åˆ°è©²ç­†äº¤æ˜“çš„é€²å ´æ©Ÿç‡èˆ‡æ–¹å‘
    df['Entry_Prob_Fixed'] = df.groupby('Code')['Entry_Prob_Fixed'].ffill()
    df['Entry_Type'] = df.groupby('Code')['Entry_Type'].ffill()
    
    # 2. éæ¿¾ï¼šåªä¿ç•™æœ‰çµç®—çš„å‡ºå ´æ—¥
    df_res = df.dropna(subset=['Conf_Weighted_Ret']).copy()
    
    if df_res.empty:
        print("No closed trades to analyze.")
        return

    # 3. å»ºç«‹æ©Ÿç‡å€é–“ (Bucket)
    df_res['Prob_Bucket'] = np.floor(df_res['Entry_Prob_Fixed'] * 100) / 100
    
    # 4. æ‹†åˆ† Long èˆ‡ Short è³‡æ–™é›†
    df_long = df_res[df_res['Entry_Type'] == 1].copy()
    df_short = df_res[df_res['Entry_Type'] == -1].copy()
    
    # --- å…§éƒ¨å‡½å¼ï¼šçµ±è¨ˆèˆ‡åˆ—å° ---
    def process_and_print(sub_df, title_prefix):
        if sub_df.empty:
            print(f"\nNo {title_prefix} trades found.")
            return None
            
        stats = sub_df.groupby('Prob_Bucket').agg({
            'Conf_Weighted_Ret': 'sum',
            'Net_Cum_Ret': 'count', # Trade Count
            # Win Rate è¨ˆç®—: Long(Confusion=1), Short(Confusion=-1)
            'Confusion': lambda x: (x.abs() == 1).sum() / x.count()
        }).rename(columns={'Net_Cum_Ret': 'Trade_Count', 'Confusion': 'Win_Rate'})
        
        stats = stats.sort_index()
        
        print(f"\n--- {title_prefix} Performance by Probability ---")
        print(f"{'Entry Prob':<15} | {'Trades':>8} | {'Total Conf_W_Ret':>18} | {'Win Rate':>10}")
        print("-" * 65)
        for prob, row in stats.iterrows():
            range_str = f"{prob:.2f} - {prob+0.01:.2f}"
            print(f"{range_str:<15} | {int(row['Trade_Count']):>8} | {row['Conf_Weighted_Ret']:>18.6f} | {row['Win_Rate']:>10.1%}")
            
        return stats

    # 5. åŸ·è¡Œçµ±è¨ˆ
    stats_long = process_and_print(df_long, "LONG (Buy)")
    stats_short = process_and_print(df_short, "SHORT (Sell)")
    
    # 6. ç¹ªåœ– (é›™å­åœ–æ¯”è¼ƒ)
    fig, axes = plt.subplots(2, 1, figsize=(12, 10), sharex=True)
    
    # Plot Long
    if stats_long is not None and not stats_long.empty:
        colors_l = ['green' if x > 0 else 'red' for x in stats_long['Conf_Weighted_Ret']]
        axes[0].bar(stats_long.index + 0.005, stats_long['Conf_Weighted_Ret'], width=0.008, color=colors_l, alpha=0.7)
        axes[0].set_title('LONG Strategy Profitability by Confidence')
        axes[0].set_ylabel('Total Conf_Weighted_Ret')
        axes[0].grid(True, alpha=0.3)
        axes[0].axhline(0, color='black', linewidth=0.8)
    
    # Plot Short
    if stats_short is not None and not stats_short.empty:
        colors_s = ['green' if x > 0 else 'red' for x in stats_short['Conf_Weighted_Ret']]
        axes[1].bar(stats_short.index + 0.005, stats_short['Conf_Weighted_Ret'], width=0.008, color=colors_s, alpha=0.7)
        axes[1].set_title('SHORT Strategy Profitability by Confidence')
        axes[1].set_xlabel('XGB Entry Probability (Confidence)')
        axes[1].set_ylabel('Total Conf_Weighted_Ret')
        axes[1].grid(True, alpha=0.3)
        axes[1].axhline(0, color='black', linewidth=0.8)
        
    plt.tight_layout()
    plt.show()

# åŸ·è¡Œåˆ†æ
if 'df_Exe' in locals() and not df_Exe.empty:
    analyze_long_short_profitability(df_Exe)
else:
    print("df_Exe not found.")


# %% [11] Analysis: Profitability by Year (Split Long vs Short) - Fixed Layout

def analyze_long_short_profitability_by_year(df_exe):
    print("\n" + "="*60)
    print("ğŸ“Š Analysis: Long vs Short Profitability by Year")
    print("="*60)
    
    df = df_exe.copy()
    if not np.issubdtype(df['Date'].dtype, np.datetime64):
        df['Date'] = pd.to_datetime(df['Date'])
        
    # 1. è³‡æ–™å‰è™•ç†
    df['Entry_Type'] = np.nan 
    entry_mask = df['Exec_Sig'].isin([1, -1])
    df.loc[entry_mask, 'Entry_Type'] = df.loc[entry_mask, 'Exec_Sig']
    df['Entry_Type'] = df.groupby('Code')['Entry_Type'].ffill()
    df['Year'] = df['Date'].dt.year
    
    # 2. éæ¿¾çµç®—äº¤æ˜“
    df_res = df.dropna(subset=['Conf_Weighted_Ret']).copy()
    
    if df_res.empty:
        print("No closed trades to analyze.")
        return

    df_long = df_res[df_res['Entry_Type'] == 1].copy()
    df_short = df_res[df_res['Entry_Type'] == -1].copy()
    
    # --- å…§éƒ¨çµ±è¨ˆå‡½å¼ ---
    def get_stats(sub_df):
        if sub_df.empty: return None
        return sub_df.groupby('Year').agg({
            'Conf_Weighted_Ret': 'sum',
            'Net_Cum_Ret': 'count',
            'Confusion': lambda x: (x.abs() == 1).sum() / x.count()
        })

    stats_long = get_stats(df_long)
    stats_short = get_stats(df_short)

    # 3. ç¹ªåœ– (å„ªåŒ–ç‰ˆ Layout)
    # å¢åŠ é«˜åº¦ä»¥é¿å…è·‘ç‰ˆ
    fig, axes = plt.subplots(2, 1, figsize=(12, 12)) 
    
    # ç•«åœ–å‡½å¼
    def plot_bars(ax, stats, title, color_logic):
        if stats is None or stats.empty:
            ax.text(0.5, 0.5, 'No Data Available', ha='center', va='center')
            return
            
        colors = [color_logic(x) for x in stats['Conf_Weighted_Ret']]
        bars = ax.bar(stats.index, stats['Conf_Weighted_Ret'], color=colors, alpha=0.7)
        
        ax.set_title(title, fontsize=14, fontweight='bold')
        ax.set_ylabel('Total Weighted Return', fontsize=12)
        ax.grid(True, axis='y', alpha=0.3, linestyle='--')
        ax.axhline(0, color='black', linewidth=1)
        
        # å¼·åˆ¶è¨­å®š X è»¸ç‚ºæ•´æ•¸å¹´ä»½ï¼Œé¿å…å‡ºç¾ 2016.5 é€™ç¨®å°æ•¸
        ax.set_xticks(stats.index)
        ax.set_xticklabels(stats.index, fontsize=11)
        
        # æ¨™ç¤ºæ•¸å€¼ (å„ªåŒ–ä½ç½®)
        y_min, y_max = ax.get_ylim()
        y_range = y_max - y_min
        
        for bar in bars:
            height = bar.get_height()
            # æ ¹æ“šæ­£è² å€¼èª¿æ•´ offset æ–¹å‘
            offset = y_range * 0.02 if height >= 0 else -y_range * 0.05
            ax.text(bar.get_x() + bar.get_width()/2., height + offset,
                    f'{height:.1f}', ha='center', va='bottom' if height > 0 else 'top', 
                    fontsize=10, fontweight='bold', color='black')

    # Plot Long
    plot_bars(axes[0], stats_long, 'LONG Strategy Profitability by Year', 
              lambda x: 'forestgreen' if x > 0 else 'firebrick')
    
    # Plot Short
    plot_bars(axes[1], stats_short, 'SHORT Strategy Profitability by Year', 
              lambda x: 'limegreen' if x > 0 else 'indianred')
        
    plt.tight_layout(pad=3.0) # å¢åŠ åœ–è¡¨é–“è·
    plt.show()
    
    # 4. åˆ—å°å ±è¡¨
    for name, stats in [("LONG (Buy)", stats_long), ("SHORT (Sell)", stats_short)]:
        if stats is not None:
            print(f"\n--- {name} Performance by Year ---")
            print(f"{'Year':<6} | {'Trades':>8} | {'Total Return':>15} | {'Win Rate':>10}")
            print("-" * 50)
            for year, row in stats.iterrows():
                print(f"{year:<6} | {int(row['Net_Cum_Ret']):>8} | {row['Conf_Weighted_Ret']:>15.4f} | {row['Confusion']:>10.1%}")

# åŸ·è¡Œåˆ†æ
if 'df_Exe' in locals() and not df_Exe.empty:
    analyze_long_short_profitability_by_year(df_Exe)
else:
    print("df_Exe not found.")

# %% [12] Analysis: Profitability by Year & Probability Bucket (Detailed)

def analyze_year_bucket_distribution(df_exe):
    print("\n" + "="*60)
    print("ğŸ“Š Analysis: Profitability by Year & 5% Probability Buckets")
    print("="*60)
    
    df = df_exe.copy()
    if not np.issubdtype(df['Date'].dtype, np.datetime64):
        df['Date'] = pd.to_datetime(df['Date'])

    # 1. è³‡æ–™å‰è™•ç†ï¼šå›æº¯é€²å ´è³‡è¨Š
    df['Entry_Prob_Fixed'] = np.nan
    df['Entry_Type'] = np.nan 
    
    entry_mask = df['Exec_Sig'].isin([1, -1])
    df.loc[entry_mask, 'Entry_Prob_Fixed'] = df.loc[entry_mask, 'Prob']
    df.loc[entry_mask, 'Entry_Type'] = df.loc[entry_mask, 'Exec_Sig']
    
    df['Entry_Prob_Fixed'] = df.groupby('Code')['Entry_Prob_Fixed'].ffill()
    df['Entry_Type'] = df.groupby('Code')['Entry_Type'].ffill()
    
    # å»ºç«‹å¹´ä»½
    df['Year'] = df['Date'].dt.year
    
    # åªä¿ç•™å·²çµç®—äº¤æ˜“
    df_res = df.dropna(subset=['Conf_Weighted_Ret']).copy()
    
    if df_res.empty:
        print("No trades found.")
        return

    # 2. å»ºç«‹ 5% Buckets (0.50, 0.55, 0.60 ...)
    # å°‡æ©Ÿç‡ç„¡æ¢ä»¶æ¨å»åˆ°å°æ•¸ç¬¬äºŒä½ï¼Œä¸¦ä»¥ 0.05 ç‚ºå–®ä½
    # ä¾‹å¦‚ 0.63 -> 0.60, 0.68 -> 0.65
    df_res['Prob_Bucket'] = (np.floor(df_res['Entry_Prob_Fixed'] / 0.05) * 0.05).round(2)
    
    # å®šç¾©æˆ‘å€‘è¦è§€å¯Ÿçš„ Bucket ç¯„åœ (å¾ 0.50 åˆ° 0.95)
    all_buckets = np.arange(0.50, 1.00, 0.05).round(2)
    years = sorted(df_res['Year'].unique())
    
    # 3. ç¹ªåœ–è¨­å®š
    fig, axes = plt.subplots(2, 1, figsize=(14, 12), sharex=True)
    
    # è¨­å®šé•·æ¢åœ–å¯¬åº¦é‚è¼¯
    # å‡è¨­ä¸€å¹´ä½”æ“š X è»¸é•·åº¦ç‚º 1.0
    # æˆ‘å€‘ç•™ 0.15 çš„é–“éš™ï¼Œå‰© 0.85 çµ¦ Bar
    # ç¸½å…±æœ‰ 10 å€‹ buckets (0.50 ~ 0.95)
    total_width = 0.85
    bar_width = total_width / len(all_buckets)
    
    # --- å…§éƒ¨ç¹ªåœ–å‡½å¼ ---
    def plot_layer(ax, entry_type, title):
        # ç¯©é¸è³‡æ–™
        sub_df = df_res[df_res['Entry_Type'] == entry_type]
        if sub_df.empty:
            ax.text(0.5, 0.5, 'No Trades', ha='center', transform=ax.transAxes)
            return

        # èšåˆæ•¸æ“š: [Year, Bucket] -> Sum Return
        agg = sub_df.groupby(['Year', 'Prob_Bucket'])['Conf_Weighted_Ret'].sum()
        
        # ç¹ªè£½åŸºæº–ç·š
        ax.axhline(0, color='black', linewidth=1, alpha=0.5)
        
        # è¿´åœˆç¹ªè£½
        # å¤–å±¤ï¼šå¹´ä»½
        for year in years:
            # å…§å±¤ï¼šæ¯ä¸€å€‹ Bucket
            for i, bucket in enumerate(all_buckets):
                if (year, bucket) in agg.index:
                    val = agg.loc[(year, bucket)]
                    
                    # è¨ˆç®— X åº§æ¨™
                    # Year æ˜¯ä¸­å¿ƒé»ï¼Œæˆ‘å€‘å…ˆç§»åˆ°æœ€å·¦é‚Šï¼Œç„¶å¾ŒåŠ ä¸Š bucket åç§»é‡
                    # offset = (i - len/2) * w
                    x_center = year
                    x_offset = (i - len(all_buckets)/2 + 0.5) * bar_width
                    x_pos = x_center + x_offset
                    
                    color = 'forestgreen' if val > 0 else 'firebrick'
                    edge_color = 'white' if abs(val) > 0 else 'none'
                    
                    # ç•« Bar (align='center')
                    # linewidth=0.5 è®“ bar ä¹‹é–“æœ‰ä¸€æ¢æ¥µç´°çš„ç™½ç·šå€éš”ï¼Œé¿å…è¦–è¦ºç³Šåœ¨ä¸€èµ·
                    ax.bar(x_pos, val, width=bar_width, color=color, edgecolor=edge_color, linewidth=0.3, alpha=0.85)

        ax.set_title(title, fontsize=14, fontweight='bold')
        ax.set_ylabel('Total Profit / Loss', fontsize=12)
        ax.grid(True, axis='y', alpha=0.2, linestyle='--')
        
        # æ¨™è¨˜å¹´ä»½åˆ†éš”ç·š (è™›ç·š)
        for y in years[:-1]:
            ax.axvline(y + 0.5, color='gray', linestyle=':', alpha=0.3)

    # 4. åŸ·è¡Œç¹ªåœ–
    plot_layer(axes[0], 1, 'LONG Strategy: PnL by Year & Confidence Bucket (Left=Low Conf, Right=High Conf)')
    plot_layer(axes[1], -1, 'SHORT Strategy: PnL by Year & Confidence Bucket')
    
    # 5. è¨­å®š X è»¸æ¨™ç±¤
    axes[1].set_xticks(years)
    axes[1].set_xticklabels(years, fontsize=12, fontweight='bold')
    axes[1].set_xlabel('Year (Internal Bars: 50% -> 95% Confidence)', fontsize=12)
    
    # å¢åŠ åœ–ä¾‹èªªæ˜ Bar çš„æ„ç¾©
    # æ‰‹å‹•å»ºç«‹ä¸€å€‹ Legend èªªæ˜ Bar çš„æ’åˆ—é †åº
    from matplotlib.lines import Line2D
    legend_elements = [
        Line2D([0], [0], color='gray', lw=0, label='Bar Order within Year:'),
        Line2D([0], [0], color='gray', lw=0, label='Left: 50% Prob'),
        Line2D([0], [0], color='gray', lw=0, label='Right: 95%+ Prob')
    ]
    axes[0].legend(handles=legend_elements, loc='upper left', frameon=True)

    plt.tight_layout()
    plt.show()

# åŸ·è¡Œ
if 'df_Exe' in locals() and not df_Exe.empty:
    analyze_year_bucket_distribution(df_Exe)
else:
    print("df_Exe not found.")



# %% [13] Visualization: Net Return Distribution

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

print("ğŸš€ ç¹ªè£½åˆ†ä½ˆåœ– (ç›´æ¥é–å®š 'Net_Ret')...")

# 1. è³‡æ–™æ¸…æ´—èˆ‡æˆªæ–· (ç›´æ¥ä½¿ç”¨ Net_Ret)
# ------------------------------------------------
# ç§»é™¤ç„¡é™å¤§èˆ‡ç©ºå€¼
clean_data = df_Inv['Net_Ret'].replace([np.inf, -np.inf], np.nan).dropna()

# å¼·åˆ¶æˆªæ–·ï¼šå°æ–¼ -1.0 çš„éƒ½è¦–ç‚º -1.0 (ç‚ºäº†å †ç–Šåœ¨æœ€å·¦é‚Š)
# å¤§æ–¼ 3.0 çš„è¦–ç‚º 3.0 (é¿å…æ¥µç«¯å€¼æ‹‰é•·åœ–è¡¨)
plot_data = clean_data.clip(lower=-1.0, upper=3.0)

# çµ±è¨ˆæ•¸æ“š
median_ret = clean_data.median()
win_rate = (clean_data > 0).mean()

# 2. ç¹ªåœ–è¨­å®š
# ------------------------------------------------
plt.figure(figsize=(14, 8))
sns.set_style("whitegrid")

# (A) ç›´æ–¹åœ–
# binrange=(-1.0, 3.0): å¼·åˆ¶ç¬¬ä¸€å€‹ bin æº–ç¢ºå¾ -1.0 é–‹å§‹
ax = sns.histplot(plot_data, bins=40, binrange=(-1.0, 3.0), kde=False, 
                  color='teal', edgecolor='white', alpha=0.85)

# (B) å¯†åº¦æ›²ç·š (KDE)
if len(plot_data) > 10:
    try:
        sns.kdeplot(plot_data, color='darkslategray', linewidth=1.5, ax=ax, cut=0)
    except:
        pass

# (C) é—œéµä¿®æ­£ï¼šå¼·åˆ¶è¨­å®š X è»¸åˆ»åº¦èˆ‡æ¨™ç±¤
# ------------------------------------------------
# 1. è¨­å®š X è»¸ç¯„åœ (å·¦é‚Šå¤šç•™ä¸€é»ç©ºéš™ï¼Œè®“ -1.0 çš„ bar ä¸æœƒè¢«åˆ‡æ‰)
ax.set_xlim(left=-1.15, right=3.1)

# 2. å¼·åˆ¶æŒ‡å®šåˆ»åº¦ä½ç½® (å¿…é ˆåŒ…å« -1.0)
custom_ticks = [-1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]
ax.set_xticks(custom_ticks)

# 3. å»ºç«‹è‡ªè¨‚æ¨™ç±¤ (è™•ç† <-100%)
custom_labels = []
for t in custom_ticks:
    if t == -1.0:
        custom_labels.append("<-100%") 
    else:
        custom_labels.append(f"{t:.0%}")
        
# 4. æ‡‰ç”¨æ¨™ç±¤ä¸¦åŠ å¤§å­—é«”
ax.set_xticklabels(custom_labels, fontweight='bold', fontsize=14)

# (D) åƒè€ƒç·šèˆ‡æ¨™è¨»
# ------------------------------------------------
plt.axvline(0, color='black', linewidth=1.5, linestyle='-', label='Break-even')
plt.axvline(median_ret, color='orange', linestyle='-', linewidth=2, label=f'Median: {median_ret:.2%}')

# çµ±è¨ˆæ¡†
stats_text = (f"Total Stocks: {len(clean_data)}\n"
              f"Median: {median_ret:.2%}\n"
              f"Win Rate: {win_rate:.2%}\n"
              f"(Bankruptcy clipped at -100%)")

props = dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor='gray')
plt.gca().text(0.98, 0.95, stats_text, transform=plt.gca().transAxes, fontsize=12,
               verticalalignment='top', horizontalalignment='right', bbox=props)

# (E) åŠ å¤§å­—é«”èˆ‡æ¨™é¡Œ
plt.title('Distribution of Net Returns (Net_Ret)', fontsize=20, fontweight='bold', pad=15)
plt.xlabel('Net Return', fontsize=16, fontweight='bold')
plt.ylabel('Frequency (Stock Count)', fontsize=16, fontweight='bold')
plt.tick_params(axis='y', labelsize=14)

plt.legend(loc='upper right', fontsize=14)
plt.tight_layout()
plt.show()

print("âœ… ç¹ªåœ–å®Œæˆ")
# clean_data = clean_data[clean_data.abs() > 1e-6]


# %% [14] Sharpe, Win Rate, and Max Drawdown


def compute_portfolio_metrics(df_exe, total_stocks_count=746, risk_free_rate=0.0):
    print("ğŸš€ è¨ˆç®—ç­‰æ¬Šé‡æŠ•è³‡çµ„åˆç¸¾æ•ˆæŒ‡æ¨™ (Win Rate, Sharpe, MDD)...")
    
    if df_exe is None or df_exe.empty:
        print("Error: df_Exe is empty.")
        return

    # ==========================================
    # 1. å»ºç«‹æŠ•è³‡çµ„åˆæ¯æ—¥å ±é…¬ç‡åºåˆ—
    # ==========================================
    df = df_exe[['Date', 'Code', 'Net_Cum_Ret', 'Close']].copy()
    df['Date'] = pd.to_datetime(df['Date'])
    
    # æ’åº
    df = df.sort_values(['Code', 'Date'])
    
    # é‚„åŸæ¯æ—¥æç›Š (Daily PnL in Points)
    df['Daily_PnL'] = df.groupby('Code')['Net_Cum_Ret'].diff().fillna(0)
    
    # ä¿®æ­£ç¬¬ä¸€ç­†äº¤æ˜“ (diff æœƒæ˜¯ 0ï¼Œéœ€è£œå›)
    mask = (df['Net_Cum_Ret'] != 0) & (df['Daily_PnL'] == 0)
    df.loc[mask, 'Daily_PnL'] = df.loc[mask, 'Net_Cum_Ret']
    
    # è½‰ç‚ºç™¾åˆ†æ¯”è²¢ç» (Contribution %)
    # å…¬å¼: (ç•¶æ—¥è³ºçš„é»æ•¸ / è‚¡åƒ¹)
    df['Daily_Contrib_Pct'] = df['Daily_PnL'] / df['Close']
    
    # èšåˆï¼šç®—å‡ºã€ŒæŠ•è³‡çµ„åˆã€æ¯ä¸€å¤©çš„ç¸½å ±é…¬ç‡
    # é€™è£¡é™¤ä»¥ total_stocks_count (746) æ˜¯ç­‰æ¬Šé‡çš„é—œéµ
    portfolio_daily_ret = df.groupby('Date')['Daily_Contrib_Pct'].sum() / total_stocks_count
    
    # ==========================================
    # 2. è¨ˆç®—ç¸¾æ•ˆæŒ‡æ¨™ (Metrics)
    # ==========================================
    
    # --- A. Win Rate (æ—¥å‹ç‡) ---
    # çµ±è¨ˆå ±é…¬ç‡ > 0 çš„å¤©æ•¸ä½”æ¯”
    winning_days = (portfolio_daily_ret > 0).sum()
    total_days = len(portfolio_daily_ret)
    win_rate = winning_days / total_days if total_days > 0 else 0
    
    # --- B. Sharpe Ratio (å¤æ™®å€¼) ---
    # å¹´åŒ–ä¿‚æ•¸é€šå¸¸è¨­ç‚º 252 (äº¤æ˜“æ—¥)
    mean_ret = portfolio_daily_ret.mean()
    std_ret = portfolio_daily_ret.std()
    
    if std_ret == 0:
        sharpe_ratio = 0
    else:
        # (å¹³å‡æ—¥å ±é…¬ - ç„¡é¢¨éšªåˆ©ç‡) / æ—¥æ³¢å‹•ç‡ * sqrt(252)
        # é€™è£¡å‡è¨­ risk_free_rate ç‚ºå¹´åŒ–ï¼Œéœ€è½‰ç‚ºæ—¥åŒ– (æˆ–ç›´æ¥å¿½ç•¥ï¼Œè¦–ç‚º 0)
        daily_rf = risk_free_rate / 252
        sharpe_ratio = ((mean_ret - daily_rf) / std_ret) * np.sqrt(252)
        
    # --- C. Maximum Drawdown (æœ€å¤§å›è½) ---
    # 1. è¨ˆç®—ç´¯ç©å ±é…¬æ›²ç·š (Cumulative Return)
    cum_ret = portfolio_daily_ret.cumsum()
    # 2. è¨ˆç®—æ­·å²æœ€é«˜é» (Running Max)
    running_max = cum_ret.cummax()
    # 3. è¨ˆç®—å›è½ (Drawdown)
    drawdown = cum_ret - running_max
    # 4. å–æœ€å°å€¼ (æœ€æ·±çš„å›è½)
    max_drawdown = drawdown.min()
    
    # --- D. å…¶ä»–è¼”åŠ©æŒ‡æ¨™ ---
    total_return = cum_ret.iloc[-1]
    annualized_return = total_return * (252 / total_days) # ç°¡å–®ä¼°ç®—
    
    # ==========================================
    # 3. è¼¸å‡ºçµæœ
    # ==========================================
    print("-" * 40)
    print(f"ğŸ“Š Portfolio Performance Metrics (Equal Weight, N={total_stocks_count})")
    print("-" * 40)
    print(f"Daily Win Rate:      {win_rate:.2%}")
    print(f"Sharpe Ratio:        {sharpe_ratio:.4f}")
    print(f"Maximum Drawdown:    {max_drawdown:.2%}")
    print("-" * 40)
    print(f"Total Return:        {total_return:.2%}")
    print(f"Daily Volatility:    {std_ret:.2%}")
    print("-" * 40)
    
    return {
        'Win_Rate': win_rate,
        'Sharpe_Ratio': sharpe_ratio,
        'Max_Drawdown': max_drawdown,
        'Total_Return': total_return,
        'Portfolio_Daily_Returns': portfolio_daily_ret
    }

# åŸ·è¡Œè¨ˆç®—
metrics = compute_portfolio_metrics(df_Exe, total_stocks_count=746)




# %% [15] Execution of Strategy

# df_Exe = pd.read_parquet("Transaction_Ledger_Constant.parquet")
# df_Inv = pd.read_csv("Strategy_Performance_XGB.csv")

def plot_stock_execution(df_exe, stock_code):
    """
    ç¹ªè£½ç‰¹å®šè‚¡ç¥¨çš„ç­–ç•¥åŸ·è¡Œåœ– (é›™ Y è»¸)ã€‚
    
    Args:
        df_exe (pd.DataFrame): åŒ…å«äº¤æ˜“ç´€éŒ„çš„ DataFrame (å¿…é ˆåŒ…å« Date, Close, Net_Cum_Ret, Exec_Sig)
        stock_code (str or int): è¦ç¹ªè£½çš„è‚¡ç¥¨ä»£ç¢¼
    """
    # 1. è³‡æ–™éæ¿¾
    # ç¢ºä¿ä»£ç¢¼æ ¼å¼ä¸€è‡´ (è½‰æˆå­—ä¸²æ¯”è¼ƒæœ€å®‰å…¨)
    df_plot = df_exe[df_exe['Code'].astype(str) == str(stock_code)].copy()
    
    if df_plot.empty:
        print(f"Error: No data found for stock code {stock_code}")
        return
    
    # ç¢ºä¿æ—¥æœŸæ ¼å¼
    if not pd.api.types.is_datetime64_any_dtype(df_plot['Date']):
        df_plot['Date'] = pd.to_datetime(df_plot['Date'])
        
    df_plot = df_plot.sort_values('Date')
    
    # 2. è¨­å®šç•«å¸ƒèˆ‡é›™è»¸
    fig, ax1 = plt.subplots(figsize=(14, 7))
    
    # --- å·¦è»¸ï¼šè‚¡åƒ¹ (Price) ---
    ax1.set_xlabel('Date', fontsize=18)
    ax1.set_ylabel('Stock Price', color='black', fontsize=18)
    # ä½¿ç”¨ç°è‰²ç·šæ¢é¡¯ç¤ºè‚¡åƒ¹ï¼Œä½œç‚ºèƒŒæ™¯åƒè€ƒ
    ax1.plot(df_plot['Date'], df_plot['Close'], color='black', alpha=1, linewidth=1, label='Close Price')
    ax1.tick_params(axis='y', labelcolor='black', labelsize=15)
    ax1.tick_params(axis='x', labelsize=15)
    
    # --- å³è»¸ï¼šç´¯ç©å ±é…¬ç‡ (Cumulative Return) ---
    ax2 = ax1.twinx()  # å»ºç«‹å…±äº« X è»¸çš„ç¬¬äºŒå€‹ Y è»¸
    ax2.set_ylabel('Cumulative Net Return', color='blue', fontsize=18)
    # ä½¿ç”¨è—è‰²å¯¦ç·šé¡¯ç¤ºç¸¾æ•ˆ
    ax2.plot(df_plot['Date'], df_plot['Net_Cum_Ret'], color='royalblue', linewidth=3, label='Net Cum Ret', alpha=1)
    ax2.tick_params(axis='y', labelcolor='blue', labelsize=15)
    
    # 3. æ¨™è¨˜é€²å‡ºå ´é» (æ¨™è¨˜åœ¨å³è»¸çš„å ±é…¬ç‡æ›²ç·šä¸Š)
    # Exec_Sig å®šç¾©: 1=Long Entry, -1=Short Entry, 2=Exit
    
    # (A) åšå¤šé€²å ´ (Long Entry) - ç¶ è‰²å‘ä¸Šä¸‰è§’å½¢
    long_entry = df_plot[df_plot['Exec_Sig'] == 1]
    ax2.scatter(long_entry['Date'], long_entry['Net_Cum_Ret'], 
                color='green', marker='^', s=100, zorder=5, label='Long Entry', alpha=0.5)

    # (B) åšç©ºé€²å ´ (Short Entry) - ç´…è‰²å‘ä¸‹ä¸‰è§’å½¢
    short_entry = df_plot[df_plot['Exec_Sig'] == -1]
    ax2.scatter(short_entry['Date'], short_entry['Net_Cum_Ret'], 
                color='red', marker='v', s=100, zorder=5, label='Short Entry', alpha=0.5)

    # (C) å‡ºå ´ (Exit) - é»‘è‰² X
    # æ³¨æ„ï¼šé€™è£¡å‡è¨­ Exec_Sig == 2 ä»£è¡¨å¹³å€‰
    exit_points = df_plot[df_plot['Exec_Sig'] == 2]
    ax2.scatter(exit_points['Date'], exit_points['Net_Cum_Ret'], 
                color='black', marker='X', s=80, zorder=5, label='Exit', alpha=0.9)

    # 4. æ ¼å¼ç¾åŒ–
    plt.title(f'Strategy Execution & Performance: {stock_code}', fontsize=20, fontweight='bold')
    
    # åˆä½µå…©å€‹è»¸çš„ Legend
    lines_1, labels_1 = ax1.get_legend_handles_labels()
    lines_2, labels_2 = ax2.get_legend_handles_labels()
    # é€™è£¡åªé¡¯ç¤ºå³è»¸ (å ±é…¬ç‡èˆ‡äº¤æ˜“é») çš„åœ–ä¾‹æœƒæ¯”è¼ƒæ¸…æ¥šï¼Œå› ç‚ºå·¦è»¸åªæ˜¯èƒŒæ™¯
    ax2.legend(lines_2, labels_2, loc='upper left', frameon=True, fancybox=True, framealpha=0.9, fontsize=18)
    
    # ==========================================
    # ğŸ”¥ğŸ”¥ğŸ”¥ é—œéµä¿®æ”¹ï¼šå°é½Šé›¶è»¸ (Align Zeros) ğŸ”¥ğŸ”¥ğŸ”¥
    # ==========================================
    
    # 1. å–å¾—ç›®å‰å³è»¸ (å ±é…¬ç‡) çš„ç¯„åœ
    y2_min, y2_max = ax2.get_ylim()
    
    # 2. è¨ˆç®—å³è»¸çš„ã€Œä¸Šæ–¹æ¯”ä¾‹ã€èˆ‡ã€Œä¸‹æ–¹æ¯”ä¾‹ã€
    #    å ±é…¬ç‡é€šå¸¸æœ‰æ­£æœ‰è² ï¼Œ0 åœ¨ä¸­é–“
    up2 = max(y2_max, 0)
    down2 = max(-y2_min, 0) # å–çµ•å°å€¼
    
    # é˜²å‘†ï¼šå¦‚æœå³è»¸å…¨æ˜¯æ­£çš„ (down2=0)ï¼Œæˆ–è€…å…¨æ˜¯è² çš„ (up2=0)
    if up2 == 0: up2 = 0.01 # é¿å…é™¤ä»¥é›¶
    
    # ç®—å‡ºæ¯”ä¾‹ ratio = ä¸‹æ–¹é•·åº¦ / ä¸Šæ–¹é•·åº¦
    ratio = down2 / up2
    
    # 3. å–å¾—å·¦è»¸ (è‚¡åƒ¹) çš„ç¯„åœ
    #    è‚¡åƒ¹é€šå¸¸éƒ½æ˜¯æ­£çš„ï¼Œæ‰€ä»¥ 0 åœ¨æœ€ä¸‹é¢
    y1_min, y1_max = ax1.get_ylim()
    up1 = max(y1_max, 0) # è‚¡åƒ¹ä¸Šæ–¹ç©ºé–“ (å°±æ˜¯æœ€é«˜åƒ¹)
    
    # 4. å¼·åˆ¶è¨­å®šå·¦è»¸çš„ä¸‹æ–¹ç©ºé–“ï¼Œä½¿å…¶æ¯”ä¾‹èˆ‡å³è»¸ä¸€è‡´
    #    new_down1 / up1 = down2 / up2  =>  new_down1 = up1 * ratio
    new_down1 = up1 * ratio
    
    # 5. è¨­å®šæ–°çš„å·¦è»¸ç¯„åœ
    #    é€™æ¨£å·¦è»¸çš„ 0 å°±æœƒè¢«ã€Œæ¨ã€åˆ°è·Ÿå³è»¸ 0 ä¸€æ¨£çš„é«˜åº¦
    ax1.set_ylim(-new_down1, up1)
    
    # ç•«ä¸€æ¢æ°´å¹³é›¶ç·šä½œç‚ºåƒè€ƒ
    ax2.axhline(0, color='black', linewidth=1, linestyle='--', alpha=0.5)

    # ==========================================
    
    
    # è¨­å®š X è»¸æ—¥æœŸæ ¼å¼
    # ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
    # ax1.xaxis.set_major_locator(mdates.MonthLocator(interval=3)) # æ¯3å€‹æœˆé¡¯ç¤ºä¸€æ¬¡
    # ğŸ”¥ğŸ”¥ğŸ”¥ é—œéµä¿®æ”¹ï¼šX è»¸åªé¡¯ç¤ºå¹´ä»½ ğŸ”¥ğŸ”¥ğŸ”¥
    # è¨­å®šæ ¼å¼ç‚º %Y (åªæœ‰å¹´ä»½)
    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
    # è¨­å®šåˆ»åº¦ç‚ºã€Œæ¯å¹´ã€é¡¯ç¤ºä¸€æ¬¡ (é¿å…å¤ªå¤šé‡è¤‡çš„å¹´ä»½)
    ax1.xaxis.set_major_locator(mdates.YearLocator())
    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)
    
    plt.grid(True, which='major', axis='both', linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.show()

# best_stock = int(df_Inv.sort_values('Net_Ret', ascending=False).iloc[0]['Code'])
# plot_stock_execution(df_Exe, best_stock)

best_stock = int(df_Inv.sort_values('Net_Ret', ascending=False).iloc[1]['Code'])
plot_stock_execution(df_Exe, best_stock)

best_stock = int(df_Inv.sort_values('Net_Ret', ascending=False).iloc[2]['Code'])
plot_stock_execution(df_Exe, best_stock)

best_stock = int(df_Inv.sort_values('Net_Ret', ascending=False).iloc[3]['Code'])
plot_stock_execution(df_Exe, best_stock)

best_stock = int(df_Inv.sort_values('Net_Ret', ascending=False).iloc[4]['Code'])
plot_stock_execution(df_Exe, best_stock)

best_stock = int(df_Inv.sort_values('Net_Ret', ascending=False).iloc[5]['Code'])
plot_stock_execution(df_Exe, best_stock)




best_stock = int(df_Inv.sort_values('Net_Ret', ascending=False).iloc[-1]['Code'])
plot_stock_execution(df_Exe, best_stock)

best_stock = int(df_Inv.sort_values('Net_Ret', ascending=False).iloc[-2]['Code'])
plot_stock_execution(df_Exe, best_stock)

best_stock = int(df_Inv.sort_values('Net_Ret', ascending=False).iloc[-3]['Code'])
plot_stock_execution(df_Exe, best_stock)

best_stock = int(df_Inv.sort_values('Net_Ret', ascending=False).iloc[-4]['Code'])
plot_stock_execution(df_Exe, best_stock)

best_stock = int(df_Inv.sort_values('Net_Ret', ascending=False).iloc[-5]['Code'])
plot_stock_execution(df_Exe, best_stock)



# %% Win Rates

(df_Inv['Net_Ret'] > 0).sum() - 2


(df_Inv['Net_Ret'] < 0).sum()


(df_Inv['Net_Ret'] == 0).sum()



268 + 183 + 295







